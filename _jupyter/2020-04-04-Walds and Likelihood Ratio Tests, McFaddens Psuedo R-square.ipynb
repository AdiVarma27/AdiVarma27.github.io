{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walds Test, Likelihood Ratio Test & McFadden's Pseudo-R2 for Logistic Regression\n",
    "\n",
    "* Logistic Regression is extensively used for its <b>Interpretability and Probabilistic outputs</b>. In this notebook, we are going to explore significance testing for Logistic Regression framework, and understand how important and impactful features can be recognized using variouis tests. \n",
    "\n",
    "\n",
    "* In all statistical libraries, we see the summary table provided for Linear and Logistic Regression models. All of them come with multiple columns, Actual coefficients, the z/t stat associated with the coefficient, the p-value, and Standard Error term. Along with it, we also see R-squared , Log-Likelihood, F-stats and other metrics, which are essential for <b>Inference and Decision making process.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_data_storage</th>\n",
       "      <th>csat_score</th>\n",
       "      <th>articles_viewed</th>\n",
       "      <th>smartphone_notifications_viewed</th>\n",
       "      <th>marketing_emails_clicked</th>\n",
       "      <th>social_media_ads_viewed</th>\n",
       "      <th>minutes_customer_support</th>\n",
       "      <th>months_active</th>\n",
       "      <th>churned</th>\n",
       "      <th>product_travel_expense_Active</th>\n",
       "      <th>product_travel_expense_Free-Trial</th>\n",
       "      <th>product_travel_expense_No</th>\n",
       "      <th>product_payroll_Active</th>\n",
       "      <th>product_payroll_Free-Trial</th>\n",
       "      <th>product_payroll_No</th>\n",
       "      <th>product_accounting_Active</th>\n",
       "      <th>product_accounting_Free-Trial</th>\n",
       "      <th>product_accounting_No</th>\n",
       "      <th>company_size_1-10</th>\n",
       "      <th>company_size_10-50</th>\n",
       "      <th>company_size_100-250</th>\n",
       "      <th>company_size_50-100</th>\n",
       "      <th>company_size_self-employed</th>\n",
       "      <th>us_region_East North Central</th>\n",
       "      <th>us_region_East South Central</th>\n",
       "      <th>us_region_Middle Atlantic</th>\n",
       "      <th>us_region_Mountain</th>\n",
       "      <th>us_region_New England</th>\n",
       "      <th>us_region_Pacific</th>\n",
       "      <th>us_region_South Atlantic</th>\n",
       "      <th>us_region_West North Central</th>\n",
       "      <th>us_region_West South Central</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>8.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5120</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_data_storage  csat_score  articles_viewed  \\\n",
       "0                  2048           9                4   \n",
       "1                  2048           9                4   \n",
       "2                  2048           9                3   \n",
       "3                   500          10                0   \n",
       "4                  5120           8                5   \n",
       "\n",
       "   smartphone_notifications_viewed  marketing_emails_clicked  \\\n",
       "0                                0                        14   \n",
       "1                                2                        12   \n",
       "2                                2                        17   \n",
       "3                                0                        14   \n",
       "4                                0                        17   \n",
       "\n",
       "   social_media_ads_viewed  minutes_customer_support  months_active  churned  \\\n",
       "0                        1                       8.3            3.0      1.0   \n",
       "1                        1                       0.0            2.0      1.0   \n",
       "2                        1                       0.0            7.0      0.0   \n",
       "3                        0                       0.0            8.0      1.0   \n",
       "4                        0                       0.0            7.0      0.0   \n",
       "\n",
       "   product_travel_expense_Active  product_travel_expense_Free-Trial  \\\n",
       "0                              0                                  1   \n",
       "1                              0                                  1   \n",
       "2                              1                                  0   \n",
       "3                              1                                  0   \n",
       "4                              0                                  1   \n",
       "\n",
       "   product_travel_expense_No  product_payroll_Active  \\\n",
       "0                          0                       1   \n",
       "1                          0                       0   \n",
       "2                          0                       1   \n",
       "3                          0                       0   \n",
       "4                          0                       1   \n",
       "\n",
       "   product_payroll_Free-Trial  product_payroll_No  product_accounting_Active  \\\n",
       "0                           0                   0                          0   \n",
       "1                           1                   0                          1   \n",
       "2                           0                   0                          1   \n",
       "3                           1                   0                          0   \n",
       "4                           0                   0                          0   \n",
       "\n",
       "   product_accounting_Free-Trial  product_accounting_No  company_size_1-10  \\\n",
       "0                              0                      1                  0   \n",
       "1                              0                      0                  0   \n",
       "2                              0                      0                  0   \n",
       "3                              0                      1                  0   \n",
       "4                              1                      0                  0   \n",
       "\n",
       "   company_size_10-50  company_size_100-250  company_size_50-100  \\\n",
       "0                   1                     0                    0   \n",
       "1                   0                     1                    0   \n",
       "2                   0                     1                    0   \n",
       "3                   0                     0                    1   \n",
       "4                   0                     0                    1   \n",
       "\n",
       "   company_size_self-employed  us_region_East North Central  \\\n",
       "0                           0                             0   \n",
       "1                           0                             0   \n",
       "2                           0                             0   \n",
       "3                           0                             0   \n",
       "4                           0                             1   \n",
       "\n",
       "   us_region_East South Central  us_region_Middle Atlantic  \\\n",
       "0                             0                          0   \n",
       "1                             0                          0   \n",
       "2                             1                          0   \n",
       "3                             1                          0   \n",
       "4                             0                          0   \n",
       "\n",
       "   us_region_Mountain  us_region_New England  us_region_Pacific  \\\n",
       "0                   0                      0                  0   \n",
       "1                   0                      0                  0   \n",
       "2                   0                      0                  0   \n",
       "3                   0                      0                  0   \n",
       "4                   0                      0                  0   \n",
       "\n",
       "   us_region_South Atlantic  us_region_West North Central  \\\n",
       "0                         0                             1   \n",
       "1                         1                             0   \n",
       "2                         0                             0   \n",
       "3                         0                             0   \n",
       "4                         0                             0   \n",
       "\n",
       "   us_region_West South Central  \n",
       "0                             0  \n",
       "1                             0  \n",
       "2                             0  \n",
       "3                             0  \n",
       "4                             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# pandas settings\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)\n",
    "\n",
    "# churn dataset\n",
    "df_churn = pd.read_csv('churn_out.csv')\n",
    "df_churn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "\n",
    "Our Dataset: Churn data from pysurvival package\n",
    "\n",
    "[A software as a service (SaaS) company provides a suite of products for Small-to-Medium enterprises, such as data storage, Accounting, Travel and Expenses management as well as Payroll management.So as to help the CFO forecast the acquisition and marketing costs for the next fiscal year, the Data Science team wants to build a churn model to predict when customers are likely to stop their monthly subscription. Thus, once customers have been flagged as likely to churn within a certain time window, the company could take the necessary retention actions.](!https://square.github.io/pysurvival/tutorials/churn.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering on churn dataset\n",
    "df_churn_ = df_churn[['product_data_storage', 'csat_score', 'articles_viewed',\n",
    "       'smartphone_notifications_viewed', 'marketing_emails_clicked',\n",
    "       'social_media_ads_viewed', 'minutes_customer_support', 'months_active',\n",
    "       'churned']]\n",
    "\n",
    "# splitting into independent and dependent variables\n",
    "X = df_churn_.drop(['churned'], axis=1)\n",
    "y = df_churn_.churned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "* We observe that there are almost equal number of people who churned and who did not, in our dataset. Looking at the summary tables below, we also see that the mean <b> product_data_storage (Amount of cloud space used in GigaBytes)</b> is lower for people who did not churn (people who left used lesser storage on average).\n",
    "\n",
    "\n",
    "* We also observe that the mean <b>csat_score (Customer satisfaction score), articles viewed, marketing emails clicked clicked, social media ads viewed and months active, are lesser</b>. We also see that <b>smartphone notofications and minutes spent with customer support</b> are higher than non-churned customers. Also, churned customers have been active for lesser duration on average than non-churned customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1068\n",
       "1.0     932\n",
       "Name: churned, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_data_storage</th>\n",
       "      <th>csat_score</th>\n",
       "      <th>articles_viewed</th>\n",
       "      <th>smartphone_notifications_viewed</th>\n",
       "      <th>marketing_emails_clicked</th>\n",
       "      <th>social_media_ads_viewed</th>\n",
       "      <th>minutes_customer_support</th>\n",
       "      <th>months_active</th>\n",
       "      <th>product_travel_expense_Active</th>\n",
       "      <th>product_travel_expense_Free-Trial</th>\n",
       "      <th>product_travel_expense_No</th>\n",
       "      <th>product_payroll_Active</th>\n",
       "      <th>product_payroll_Free-Trial</th>\n",
       "      <th>product_payroll_No</th>\n",
       "      <th>product_accounting_Active</th>\n",
       "      <th>product_accounting_Free-Trial</th>\n",
       "      <th>product_accounting_No</th>\n",
       "      <th>company_size_1-10</th>\n",
       "      <th>company_size_10-50</th>\n",
       "      <th>company_size_100-250</th>\n",
       "      <th>company_size_50-100</th>\n",
       "      <th>company_size_self-employed</th>\n",
       "      <th>us_region_East North Central</th>\n",
       "      <th>us_region_East South Central</th>\n",
       "      <th>us_region_Middle Atlantic</th>\n",
       "      <th>us_region_Mountain</th>\n",
       "      <th>us_region_New England</th>\n",
       "      <th>us_region_Pacific</th>\n",
       "      <th>us_region_South Atlantic</th>\n",
       "      <th>us_region_West North Central</th>\n",
       "      <th>us_region_West South Central</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>churned</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1547.314607</td>\n",
       "      <td>9.388577</td>\n",
       "      <td>4.087079</td>\n",
       "      <td>0.33427</td>\n",
       "      <td>16.308052</td>\n",
       "      <td>0.439139</td>\n",
       "      <td>1.094101</td>\n",
       "      <td>4.751873</td>\n",
       "      <td>0.227528</td>\n",
       "      <td>0.769663</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.603933</td>\n",
       "      <td>0.350187</td>\n",
       "      <td>0.045880</td>\n",
       "      <td>0.561798</td>\n",
       "      <td>0.262172</td>\n",
       "      <td>0.176030</td>\n",
       "      <td>0.162921</td>\n",
       "      <td>0.352996</td>\n",
       "      <td>0.10206</td>\n",
       "      <td>0.350187</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.105805</td>\n",
       "      <td>0.107678</td>\n",
       "      <td>0.125468</td>\n",
       "      <td>0.117978</td>\n",
       "      <td>0.117041</td>\n",
       "      <td>0.113296</td>\n",
       "      <td>0.113296</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>0.098315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1231.394850</td>\n",
       "      <td>8.507511</td>\n",
       "      <td>3.883047</td>\n",
       "      <td>0.44206</td>\n",
       "      <td>16.008584</td>\n",
       "      <td>0.310086</td>\n",
       "      <td>3.901288</td>\n",
       "      <td>2.879828</td>\n",
       "      <td>0.140558</td>\n",
       "      <td>0.843348</td>\n",
       "      <td>0.016094</td>\n",
       "      <td>0.310086</td>\n",
       "      <td>0.377682</td>\n",
       "      <td>0.312232</td>\n",
       "      <td>0.439914</td>\n",
       "      <td>0.222103</td>\n",
       "      <td>0.337983</td>\n",
       "      <td>0.152361</td>\n",
       "      <td>0.335837</td>\n",
       "      <td>0.14485</td>\n",
       "      <td>0.335837</td>\n",
       "      <td>0.031116</td>\n",
       "      <td>0.105150</td>\n",
       "      <td>0.110515</td>\n",
       "      <td>0.108369</td>\n",
       "      <td>0.116953</td>\n",
       "      <td>0.116953</td>\n",
       "      <td>0.103004</td>\n",
       "      <td>0.111588</td>\n",
       "      <td>0.116953</td>\n",
       "      <td>0.110515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         product_data_storage  csat_score  articles_viewed  \\\n",
       "churned                                                      \n",
       "0.0               1547.314607    9.388577         4.087079   \n",
       "1.0               1231.394850    8.507511         3.883047   \n",
       "\n",
       "         smartphone_notifications_viewed  marketing_emails_clicked  \\\n",
       "churned                                                              \n",
       "0.0                              0.33427                 16.308052   \n",
       "1.0                              0.44206                 16.008584   \n",
       "\n",
       "         social_media_ads_viewed  minutes_customer_support  months_active  \\\n",
       "churned                                                                     \n",
       "0.0                     0.439139                  1.094101       4.751873   \n",
       "1.0                     0.310086                  3.901288       2.879828   \n",
       "\n",
       "         product_travel_expense_Active  product_travel_expense_Free-Trial  \\\n",
       "churned                                                                     \n",
       "0.0                           0.227528                           0.769663   \n",
       "1.0                           0.140558                           0.843348   \n",
       "\n",
       "         product_travel_expense_No  product_payroll_Active  \\\n",
       "churned                                                      \n",
       "0.0                       0.002809                0.603933   \n",
       "1.0                       0.016094                0.310086   \n",
       "\n",
       "         product_payroll_Free-Trial  product_payroll_No  \\\n",
       "churned                                                   \n",
       "0.0                        0.350187            0.045880   \n",
       "1.0                        0.377682            0.312232   \n",
       "\n",
       "         product_accounting_Active  product_accounting_Free-Trial  \\\n",
       "churned                                                             \n",
       "0.0                       0.561798                       0.262172   \n",
       "1.0                       0.439914                       0.222103   \n",
       "\n",
       "         product_accounting_No  company_size_1-10  company_size_10-50  \\\n",
       "churned                                                                 \n",
       "0.0                   0.176030           0.162921            0.352996   \n",
       "1.0                   0.337983           0.152361            0.335837   \n",
       "\n",
       "         company_size_100-250  company_size_50-100  \\\n",
       "churned                                              \n",
       "0.0                   0.10206             0.350187   \n",
       "1.0                   0.14485             0.335837   \n",
       "\n",
       "         company_size_self-employed  us_region_East North Central  \\\n",
       "churned                                                             \n",
       "0.0                        0.031835                      0.105805   \n",
       "1.0                        0.031116                      0.105150   \n",
       "\n",
       "         us_region_East South Central  us_region_Middle Atlantic  \\\n",
       "churned                                                            \n",
       "0.0                          0.107678                   0.125468   \n",
       "1.0                          0.110515                   0.108369   \n",
       "\n",
       "         us_region_Mountain  us_region_New England  us_region_Pacific  \\\n",
       "churned                                                                 \n",
       "0.0                0.117978               0.117041           0.113296   \n",
       "1.0                0.116953               0.116953           0.103004   \n",
       "\n",
       "         us_region_South Atlantic  us_region_West North Central  \\\n",
       "churned                                                           \n",
       "0.0                      0.113296                      0.101124   \n",
       "1.0                      0.111588                      0.116953   \n",
       "\n",
       "         us_region_West South Central  \n",
       "churned                                \n",
       "0.0                          0.098315  \n",
       "1.0                          0.110515  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_churn.groupby('churned').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model (Predicting Churn Propensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.776, 0.757)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# fitting logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "LRModel = LogisticRegression(max_iter=500).fit(X_train, y_train)\n",
    "y_pred = LRModel.predict(X_test)\n",
    "\n",
    "# training and testing accuracies\n",
    "accuracy_score(LRModel.predict(X_train), y_train), accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Contributions\n",
    "\n",
    "* The table below contains Coefficients of all attributes along with the model intercept. We know that Logistic Regression models log of odds as linear combination of coefficients and the data. The expression can be re-written in terms of Odds, by multiplying with exponent on both sides; which provides Odds of Churning to Non-Churning.\n",
    "\n",
    "\n",
    "$$log\\frac{p(y)}{1-p(y)} = \\beta{_0} + \\beta{_1}x{_1} + \\beta{_2}x{_2} + ... + \\beta{_n}x{_n}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{p(y)}{1-p(y)} = \\exp^{\\beta{_0} + \\beta{_1}x{_1} + \\beta{_2}x{_2} + ... + \\beta{_n}x{_n}}$$\n",
    "\n",
    "\n",
    "* One unit increase/ decrease of an attribute (X), leads to 'Beta' increase/ decrease in Log odds of event occurance. In terms of Odds, it is increase/ decrease of Exponent of 'Beta'. Hence, we convert Log-Odds to True-Odds by applying an exponential transformation as shown in table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>beta</th>\n",
       "      <th>odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intercept_term</td>\n",
       "      <td>11.825976</td>\n",
       "      <td>136759.05228577272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product_data_storage</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>0.9997354906118859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csat_score</td>\n",
       "      <td>-1.114814</td>\n",
       "      <td>0.3279764392254565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>articles_viewed</td>\n",
       "      <td>-0.042609</td>\n",
       "      <td>0.958286007085567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smartphone_notifications_viewed</td>\n",
       "      <td>0.388169</td>\n",
       "      <td>1.4742782682460098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>marketing_emails_clicked</td>\n",
       "      <td>-0.037101</td>\n",
       "      <td>0.9635785556193255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>social_media_ads_viewed</td>\n",
       "      <td>-0.391455</td>\n",
       "      <td>0.6760726937037167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>minutes_customer_support</td>\n",
       "      <td>0.089820</td>\n",
       "      <td>1.093977080726584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>months_active</td>\n",
       "      <td>-0.275176</td>\n",
       "      <td>0.7594381755978886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          features       beta                odds\n",
       "0                   intercept_term  11.825976  136759.05228577272\n",
       "1             product_data_storage  -0.000265  0.9997354906118859\n",
       "2                       csat_score  -1.114814  0.3279764392254565\n",
       "3                  articles_viewed  -0.042609   0.958286007085567\n",
       "4  smartphone_notifications_viewed   0.388169  1.4742782682460098\n",
       "5         marketing_emails_clicked  -0.037101  0.9635785556193255\n",
       "6          social_media_ads_viewed  -0.391455  0.6760726937037167\n",
       "7         minutes_customer_support   0.089820   1.093977080726584\n",
       "8                    months_active  -0.275176  0.7594381755978886"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat = pd.DataFrame()\n",
    "df_feat['features'] = ['intercept_term'] + X.columns.tolist()\n",
    "df_feat['beta'] = [LRModel.intercept_] + LRModel.coef_[0].tolist()\n",
    "df_feat.loc[0, 'beta'] = df_feat.iloc[0].beta[0]\n",
    "df_feat['beta'] = pd.to_numeric(df_feat['beta'])\n",
    "df_feat['odds'] = np.exp(df_feat.beta)\n",
    "df_feat['odds'] = df_feat['odds'].astype('str')\n",
    "df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we reduce churn ?\n",
    "\n",
    "\n",
    "### Media Targeting Strategy\n",
    "\n",
    "* Smart phone notifications viewed have higher odds for churned vs non-churned customers, which could <b>possibly indicate unnecessary notifications popping up to customers, it could also be irrelevant notifications being displayed.</b> We could design experiments to see what the exact issue (Could be irrelevant notifications, or just the frequency of notifications). \n",
    "\n",
    "\n",
    "\n",
    "* Social Media Ads viewed have lower odds for chunred to non-churned, Hence, <b>we could design an experiment for re-targeting existing cutomers</b> (to either solidify belief of social presence or their usage of our tool/ product).\n",
    "\n",
    "\n",
    "\n",
    "### Customer Satisfaction Exploration\n",
    "* Customer satisfaction scores are clearly indicative of churn beahviour. Clearly, we need to dive deep into issues customers ran into and make sure they are covered (Also indicated by minutes_customer_support).\n",
    "\n",
    "### How Strong is the evidence for the above Recommendations ?\n",
    "\n",
    "\n",
    "* While interpreting results of Logistic Regression Model, understanding the confidence and stability of coefficients is important. In some cases, the coefficient contribution could be high in magnitude, but have <b>high standard errors</b> associated to it. \n",
    "\n",
    "\n",
    "* In Linear Regression, R-squared term is used to explain the amount of variance captured by regression to total amount of variance, and can be simply put into a value from 0 to 1. Similarly, we can use McFaddens Pseudo R-squared to <b>describe the quality of signal in Logistic Regression</b>.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McFadden's Pseudo R-Square\n",
    "\n",
    "* Formally, McFaddens Pseudo R-square is defined as (1 - Ratio of Log-Likelihood of complete Logistic Model and Log-likelihood of only intercept term in the model). The value ranges from 0 to 1; the essence being that <b>if the model were not predictive, the LogLikelihood of Null Model and Full Model would be similar, reducing the value close to 0</b>.\n",
    "\n",
    "\n",
    "* As we know the coefficients, we can calculate the Log-Likelihood of the complete model by plugging predicted class 1 probabilities into the cost function. We find the Log Likelihood for the full model to be around -488.9 manually. Below, we use statsmodels api to find out Log-Likelihood of Full model and Null model.\n",
    "\n",
    "$$ R^2 = 1 - \\frac{LL_{Full Model}}{LL_{Intercept}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$ Cost = ylog(prediction prob) + (1-y)log(1-prediction prob)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-488.9079452207212"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "y_pred_proba = LRModel.predict_proba(X_train)[:,1]\n",
    "\n",
    "LL = np.sum(y_train*np.log(y_pred_proba) + (1-y_train)*np.log(1-y_pred_proba))\n",
    "LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.488879\n",
      "         Iterations 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya.kalidindi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>churned</td>     <th>  No. Observations:  </th>  <td>  1000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   991</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     8</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sat, 04 Apr 2020</td> <th>  Pseudo R-squ.:     </th>  <td>0.2925</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>20:45:13</td>     <th>  Log-Likelihood:    </th> <td> -488.88</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -690.97</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>2.394e-82</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>product_data_storage</th>            <td>   -0.0003</td> <td> 7.48e-05</td> <td>   -3.531</td> <td> 0.000</td> <td>   -0.000</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>csat_score</th>                      <td>   -1.1351</td> <td>    0.102</td> <td>  -11.132</td> <td> 0.000</td> <td>   -1.335</td> <td>   -0.935</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>articles_viewed</th>                 <td>   -0.0441</td> <td>    0.040</td> <td>   -1.089</td> <td> 0.276</td> <td>   -0.123</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>smartphone_notifications_viewed</th> <td>    0.3978</td> <td>    0.137</td> <td>    2.894</td> <td> 0.004</td> <td>    0.128</td> <td>    0.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marketing_emails_clicked</th>        <td>   -0.0352</td> <td>    0.026</td> <td>   -1.370</td> <td> 0.171</td> <td>   -0.085</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>social_media_ads_viewed</th>         <td>   -0.4051</td> <td>    0.151</td> <td>   -2.686</td> <td> 0.007</td> <td>   -0.701</td> <td>   -0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>minutes_customer_support</th>        <td>    0.0907</td> <td>    0.016</td> <td>    5.723</td> <td> 0.000</td> <td>    0.060</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>months_active</th>                   <td>   -0.2757</td> <td>    0.041</td> <td>   -6.645</td> <td> 0.000</td> <td>   -0.357</td> <td>   -0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept_</th>                      <td>   11.9827</td> <td>    1.036</td> <td>   11.567</td> <td> 0.000</td> <td>    9.952</td> <td>   14.013</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                churned   No. Observations:                 1000\n",
       "Model:                          Logit   Df Residuals:                      991\n",
       "Method:                           MLE   Df Model:                            8\n",
       "Date:                Sat, 04 Apr 2020   Pseudo R-squ.:                  0.2925\n",
       "Time:                        20:45:13   Log-Likelihood:                -488.88\n",
       "converged:                       True   LL-Null:                       -690.97\n",
       "Covariance Type:            nonrobust   LLR p-value:                 2.394e-82\n",
       "===================================================================================================\n",
       "                                      coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "product_data_storage               -0.0003   7.48e-05     -3.531      0.000      -0.000      -0.000\n",
       "csat_score                         -1.1351      0.102    -11.132      0.000      -1.335      -0.935\n",
       "articles_viewed                    -0.0441      0.040     -1.089      0.276      -0.123       0.035\n",
       "smartphone_notifications_viewed     0.3978      0.137      2.894      0.004       0.128       0.667\n",
       "marketing_emails_clicked           -0.0352      0.026     -1.370      0.171      -0.085       0.015\n",
       "social_media_ads_viewed            -0.4051      0.151     -2.686      0.007      -0.701      -0.110\n",
       "minutes_customer_support            0.0907      0.016      5.723      0.000       0.060       0.122\n",
       "months_active                      -0.2757      0.041     -6.645      0.000      -0.357      -0.194\n",
       "intercept_                         11.9827      1.036     11.567      0.000       9.952      14.013\n",
       "===================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#including intercept term\n",
    "X_train['intercept_'] = 1\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "mod = sm.Logit(y_train, X_train)\n",
    "results = mod.fit(maxiter=120)\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We observe that LL-Null Model us -690 and LL-Full Model is -488.88 using statsmodel api (which is equal to our manual prediction step). We see from the summary table above and step below, with <b>R-squared value of 0.2925</b>. Note: [Values of 0.2 to 0.4 show excellent fit.](http://cowles.yale.edu/sites/default/files/files/pub/d04/d0474.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2924729004153581"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LL_null_model = -690.97\n",
    "LL_full_model = -488.88\n",
    "\n",
    "R2 = 1 - (LL_full_model/LL_null_model)\n",
    "R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the overall predictability power of the model with McFaddens R-squared, Lets take a look at individual attributes and their coefficients. The Z-value in statsapi model and many other software packages referrs to <b>Wald's Tests </b>\n",
    "\n",
    "# Wald's Test\n",
    "\n",
    "* The null hypothesis for Walds test assumes no effect of variable on Log Odds of the model, and alternate hypothesis assumes there is a siginificant effect of the attribute on the model. Walds can be defined in Chi-squared or Normal Distribution. In this case, let us stick to the normal distribution formula for Walds Stat.\n",
    "\n",
    "\n",
    "* We know that the Log-Likelihood associated Logistic Regression is highest at MLE of the corresponding data and coefficients combination. Hence, on the X-axis we observe the coefficient we are testing, and y-axis is the log-likelihood associated with it.\n",
    "\n",
    "$$ MLE = \\prod_{i=1}^{n} (p)^{y}(1-p)^{1-y} = \\prod_{i=1}^{n} \\sigma(W^tX)^{y}(1-\\sigma(W^tX))^{1-y} $$\n",
    "\n",
    "\n",
    "$$ Z (Walds) =  \\frac{\\beta}{SE(\\beta)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartphone_notifications_viewed_optimal = 0.388169"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood at Optimal Beta\n",
    "\n",
    "* See below, we observe that at optimal beta for smartphone_notifications_viewed_optimal, is at 0.388169, with a log likelihood of -488.9079452207212."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya.kalidindi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-488.9079452207212"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# including intercept term in Coefficients\n",
    "\n",
    "X_train['intercept_'] = 1\n",
    "betas = np.array(list(LRModel.coef_[0]) + [LRModel.intercept_[0]])\n",
    "\n",
    "y_pred_proba = sigmoid(X_train.dot(betas))\n",
    "LL_optimal = np.sum(y_train*np.log(y_pred_proba) + (1-y_train)*np.log(1-y_pred_proba))\n",
    "LL_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood for range of Beta\n",
    "\n",
    "* Now, let us see how log-likelihood changes as we try a range of values for Beta (smartphone_notifications_viewed_optimal), with range of values on X-axis and Log-likelihood on the y-axis. We see that Log-likelihood peaks at 0.388."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(-3, 3, 0.1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya.kalidindi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-856.5008925854107,\n",
       " -839.0047094192732,\n",
       " -821.71758495081,\n",
       " -804.6528401921985,\n",
       " -787.8244032194461,\n",
       " -771.2467689409955,\n",
       " -754.9349539061101,\n",
       " -738.9044544100325,\n",
       " -723.1712179973542,\n",
       " -707.751638973766]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLs = []\n",
    "\n",
    "for i in range(0,len(np.arange(-3, 3, 0.1))):\n",
    "    X_train['intercept_'] = 1\n",
    "    betas = np.array(list(LRModel.coef_[0]) + [LRModel.intercept_[0]])\n",
    "\n",
    "\n",
    "    betas[3] = np.arange(-3, 3, 0.1)[i]\n",
    "    y_pred_proba = sigmoid(X_train.dot(betas))\n",
    "    LL = np.sum(y_train*np.log(y_pred_proba) + (1-y_train)*np.log(1-y_pred_proba))\n",
    "    LLs.append(LL)\n",
    "\n",
    "LLs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range of Betas vs Log-Likelihood\n",
    "\n",
    "* From the plot below, we observe the Green line, at optimal Beta, Log-Likelihood is highest. However, the hypothesis checks if has a significant effect on Log-Odds. The difference on X-axis between the green and red lines, divided by the Standard Error of Beta, is the Wald's Z parameter.\n",
    "\n",
    "\n",
    "* As the direction of green-vector shows, the more distant Optimal Beta is, higher the Z-stat (given SE is constant), as Z depends on Beta and Standard Error of Beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.grid()\n",
    "plt.xlabel(\"Range of Betas for 'smartphone_notifications_viewed_optimal'\")\n",
    "plt.ylabel('Log Likelihood')\n",
    "\n",
    "plt.plot(np.arange(-3, 3, 0.1), LLs)\n",
    "\n",
    "plt.scatter(smartphone_notifications_viewed_optimal, LL_optimal, color='g', marker='o', lw=5)\n",
    "plt.scatter(0, LL_optimal-5, color='r', marker='o', lw=5)\n",
    "\n",
    "plt.axvline(x=smartphone_notifications_viewed_optimal, color='g', label='Significant Effect')\n",
    "plt.axvline(x=0, color='r', label='No Effect')\n",
    "\n",
    "origin = [0], [-800] \n",
    "\n",
    "plt.quiver(*origin, 1.3,0, color=['g'], scale=21)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sometimes, we might observe that for a given cofficient, Log-likelihood could peak at Beta=0, signifying that the coefficient has no effect on prediction. In ther above case, it does make a significant difference (Walds Z of 2.83)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "pred_proba = LRModel.predict_proba(X_train)\n",
    "pred_proba_cls1 = pred_proba[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture26.pdf (page 2)\n",
    "\n",
    "\n",
    "\n",
    "$$ Var(\\beta) = X_TVX^{-1}$$\n",
    "\n",
    "\n",
    "$$ SE(\\beta) = \\sqrt{Var(\\beta)} $$\n",
    "\n",
    "where X is the design matrix (Data matrix inlcuding addiitonal column for intercept), V is the p*(1-p) diagonal matrix (due to multiple binomial trails assumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p(1-p)\n",
    "mul_pred_proba = pred_proba_cls1*(1-pred_proba_cls1)\n",
    "\n",
    "# diagnal matrix of p(1-p)\n",
    "V = np.diag(mul_pred_proba)\n",
    "\n",
    "# Design Matrix\n",
    "X = np.matrix(X_train)\n",
    "\n",
    "# Adding data for intercept term\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Variance\n",
    "Var = np.linalg.inv(np.dot(X.T, V).dot(X))\n",
    "\n",
    "# diagonalizing variance\n",
    "Var_diag = np.diag(Var)\n",
    "\n",
    "# Standard Errors\n",
    "SE = np.sqrt(Var_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All diagonal terms of the matrix correspond to variance and all off-diagonal are covariances. We include these into our feature summary tables below and try to understand them. We also observe that the results we obtained are consistent with the statsmodel api summary table above.\n",
    "\n",
    "\n",
    "* We observe that <b>articles_viewed</b> and <b>marketing_emails_clicked</b> attributes have odds of 0.95 and 0.96 respectively, which means higher the value of articles viewed or marketing emails clicked, lower the chance of churn. However, the corresponding Walds Z values are <b>-1.05</b> and <b>-1.45</b>, which are statistically insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>beta</th>\n",
       "      <th>odds</th>\n",
       "      <th>SE</th>\n",
       "      <th>Walds Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intercept_term</td>\n",
       "      <td>11.825976</td>\n",
       "      <td>136759.05228577272</td>\n",
       "      <td>1.028866</td>\n",
       "      <td>11.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product_data_storage</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>0.9997354906118859</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>-3.548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csat_score</td>\n",
       "      <td>-1.114814</td>\n",
       "      <td>0.3279764392254565</td>\n",
       "      <td>0.101158</td>\n",
       "      <td>-11.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>articles_viewed</td>\n",
       "      <td>-0.042609</td>\n",
       "      <td>0.958286007085567</td>\n",
       "      <td>0.040354</td>\n",
       "      <td>-1.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smartphone_notifications_viewed</td>\n",
       "      <td>0.388169</td>\n",
       "      <td>1.4742782682460098</td>\n",
       "      <td>0.136885</td>\n",
       "      <td>2.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>marketing_emails_clicked</td>\n",
       "      <td>-0.037101</td>\n",
       "      <td>0.9635785556193255</td>\n",
       "      <td>0.025565</td>\n",
       "      <td>-1.451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>social_media_ads_viewed</td>\n",
       "      <td>-0.391455</td>\n",
       "      <td>0.6760726937037167</td>\n",
       "      <td>0.150134</td>\n",
       "      <td>-2.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>minutes_customer_support</td>\n",
       "      <td>0.089820</td>\n",
       "      <td>1.093977080726584</td>\n",
       "      <td>0.015786</td>\n",
       "      <td>5.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>months_active</td>\n",
       "      <td>-0.275176</td>\n",
       "      <td>0.7594381755978886</td>\n",
       "      <td>0.041341</td>\n",
       "      <td>-6.656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          features       beta                odds        SE  \\\n",
       "0                   intercept_term  11.825976  136759.05228577272  1.028866   \n",
       "1             product_data_storage  -0.000265  0.9997354906118859  0.000075   \n",
       "2                       csat_score  -1.114814  0.3279764392254565  0.101158   \n",
       "3                  articles_viewed  -0.042609   0.958286007085567  0.040354   \n",
       "4  smartphone_notifications_viewed   0.388169  1.4742782682460098  0.136885   \n",
       "5         marketing_emails_clicked  -0.037101  0.9635785556193255  0.025565   \n",
       "6          social_media_ads_viewed  -0.391455  0.6760726937037167  0.150134   \n",
       "7         minutes_customer_support   0.089820   1.093977080726584  0.015786   \n",
       "8                    months_active  -0.275176  0.7594381755978886  0.041341   \n",
       "\n",
       "   Walds Z  \n",
       "0   11.494  \n",
       "1   -3.548  \n",
       "2  -11.021  \n",
       "3   -1.056  \n",
       "4    2.836  \n",
       "5   -1.451  \n",
       "6   -2.607  \n",
       "7    5.690  \n",
       "8   -6.656  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat['SE'] = SE\n",
    "df_feat['Walds Z'] = np.round((df_feat.beta/SE),3)\n",
    "# df_feat['Walds Z'] = df_feat['Walds Z'].astype('str')\n",
    "\n",
    "df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need likelihood ratio test when we already have Walds Z-estimate ?\n",
    "\n",
    "* Walds test looks for significant features, contributing to predictions. However, we could also test the hypothesis if there is a more complex decision boundary which could be captured by the data attributes. \n",
    "\n",
    "\n",
    "* Essentially, Walds and Likelihood Ratio Tests are testing for <b>different hypotheses</b>. Walds tests for how <b>further away</b> the coefficient is from (X=0), whereas Likelihood Ratio test is testing if the underlying data can find better decision boundary by including the variable vs not including the variable. \n",
    "\n",
    "\n",
    "* Using the Likelihood ratio test, we can see the direct contribution of a variable being included in a model vs not being included in the Likelihoods. Previously, Walds test was looking for <b>values further away, along the X-direction, whereas Likelihood Test is looking for gain in height (or Likelihood)</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood Ratio Test\n",
    "\n",
    "* Below, we inlcude a new attribute for multiplying with Model intercept, called 'intercept_' and set it to 1. Function LLwithoutBeta(X_, LRModel, remove_val_ind) calculates the LL of the model after replacing the beta value by 0 for each index. \n",
    "\n",
    "\n",
    "* Hence, for each attribute, we have two models (with and without including the attribute in the model), to understand its contribution to Log-Likelihood. It can also be used as a feature selection method to see if there is a significant decrease in Log-Likelihood and then decide to inlcude the model or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_data_storage</th>\n",
       "      <th>csat_score</th>\n",
       "      <th>articles_viewed</th>\n",
       "      <th>smartphone_notifications_viewed</th>\n",
       "      <th>marketing_emails_clicked</th>\n",
       "      <th>social_media_ads_viewed</th>\n",
       "      <th>minutes_customer_support</th>\n",
       "      <th>months_active</th>\n",
       "      <th>intercept_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>2048</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>5120</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2048</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>23.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_data_storage  csat_score  articles_viewed  \\\n",
       "440                  2048           9                3   \n",
       "573                   200          10                3   \n",
       "946                  5120          10                4   \n",
       "997                  2048           7                7   \n",
       "503                   500          10                4   \n",
       "\n",
       "     smartphone_notifications_viewed  marketing_emails_clicked  \\\n",
       "440                                2                        16   \n",
       "573                                0                        18   \n",
       "946                                0                        14   \n",
       "997                                0                        12   \n",
       "503                                0                        14   \n",
       "\n",
       "     social_media_ads_viewed  minutes_customer_support  months_active  \\\n",
       "440                        0                      54.0            3.0   \n",
       "573                        1                       0.0           11.0   \n",
       "946                        1                       0.0            8.0   \n",
       "997                        0                       0.0            3.0   \n",
       "503                        1                      23.5            1.0   \n",
       "\n",
       "     intercept_  \n",
       "440           1  \n",
       "573           1  \n",
       "946           1  \n",
       "997           1  \n",
       "503           1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas = np.array(list(LRModel.coef_[0]) + [LRModel.intercept_[0]]).tolist()\n",
    "X_ = X_train\n",
    "X_['intercept_'] = 1\n",
    "\n",
    "X_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function return LL\n",
    "def LLwithoutBeta(X_, LRModel, remove_val_ind):\n",
    "    \n",
    "    # extract betas\n",
    "    betas = np.array(list(LRModel.coef_[0]) + [LRModel.intercept_[0]]).tolist()\n",
    "    \n",
    "    # replace index with 0\n",
    "    betas[remove_val_ind] = 0\n",
    "\n",
    "    # predict_proba\n",
    "    y_pred_proba = sigmoid(X_.dot(betas))\n",
    "    \n",
    "    # calculating LL\n",
    "    LL = np.sum(y_train*np.log(y_pred_proba) + (1-y_train)*np.log(1-y_pred_proba))\n",
    "    \n",
    "    return LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-505.919116278803,\n",
       " -5016.28958241602,\n",
       " -491.9406903918843,\n",
       " -494.98990587555096,\n",
       " -519.1709459503775,\n",
       " -494.1320766426853,\n",
       " -516.192674250097,\n",
       " -613.8030644742258,\n",
       " -5087.135941036749]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLs = [LLwithoutBeta(X_, LRModel, val) for val in range(0,len(betas))]\n",
    "\n",
    "LLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Log-Likelihood at most optimal Betas is -488.9079452207212."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-488.9079452207212"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# including intercept term in Coefficients\n",
    "\n",
    "X_train['intercept_'] = 1\n",
    "betas = np.array(list(LRModel.coef_[0]) + [LRModel.intercept_[0]])\n",
    "\n",
    "y_pred_proba = sigmoid(X_train.dot(betas))\n",
    "LL_optimal = np.sum(y_train*np.log(y_pred_proba) + (1-y_train)*np.log(1-y_pred_proba))\n",
    "LL_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Likelihood Ratio and associated p-value\n",
    "\n",
    "Likelihood ratio is defined as twice the ratio of LogLikelihood of full model vs LogLikelihood of model without the coefficient, and has an asymptotic Chi-Squared distribution under the null hypothesis.\n",
    "\n",
    "$$ LR = 2(LL_{optimal} - LL_{null})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Likelihood Ratios for all coefficients\n",
    "LikelihoodRatio = [np.round(2*(LL_optimal - LLs[i]),2) for i in range(0,len(betas))]\n",
    "df_feat['LikelihoodRatio-t'] = LikelihoodRatio\n",
    "\n",
    "# obtaining p-values for the Chi-squared distribution\n",
    "from scipy.stats import chi2\n",
    "p_LR = [np.round(1 - chi2.cdf(val,1),3) for val in LikelihoodRatio]\n",
    "df_feat['p_value_LR'] = p_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>beta</th>\n",
       "      <th>odds</th>\n",
       "      <th>SE</th>\n",
       "      <th>Walds Z</th>\n",
       "      <th>LikelihoodRatio-t</th>\n",
       "      <th>p_value_LR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intercept_term</td>\n",
       "      <td>11.825976</td>\n",
       "      <td>136759.05228577272</td>\n",
       "      <td>1.028866</td>\n",
       "      <td>11.494</td>\n",
       "      <td>34.02</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product_data_storage</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>0.9997354906118859</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>-3.548</td>\n",
       "      <td>9054.76</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>csat_score</td>\n",
       "      <td>-1.114814</td>\n",
       "      <td>0.3279764392254565</td>\n",
       "      <td>0.101158</td>\n",
       "      <td>-11.021</td>\n",
       "      <td>6.07</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>articles_viewed</td>\n",
       "      <td>-0.042609</td>\n",
       "      <td>0.958286007085567</td>\n",
       "      <td>0.040354</td>\n",
       "      <td>-1.056</td>\n",
       "      <td>12.16</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smartphone_notifications_viewed</td>\n",
       "      <td>0.388169</td>\n",
       "      <td>1.4742782682460098</td>\n",
       "      <td>0.136885</td>\n",
       "      <td>2.836</td>\n",
       "      <td>60.53</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>marketing_emails_clicked</td>\n",
       "      <td>-0.037101</td>\n",
       "      <td>0.9635785556193255</td>\n",
       "      <td>0.025565</td>\n",
       "      <td>-1.451</td>\n",
       "      <td>10.45</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>social_media_ads_viewed</td>\n",
       "      <td>-0.391455</td>\n",
       "      <td>0.6760726937037167</td>\n",
       "      <td>0.150134</td>\n",
       "      <td>-2.607</td>\n",
       "      <td>54.57</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>minutes_customer_support</td>\n",
       "      <td>0.089820</td>\n",
       "      <td>1.093977080726584</td>\n",
       "      <td>0.015786</td>\n",
       "      <td>5.690</td>\n",
       "      <td>249.79</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>months_active</td>\n",
       "      <td>-0.275176</td>\n",
       "      <td>0.7594381755978886</td>\n",
       "      <td>0.041341</td>\n",
       "      <td>-6.656</td>\n",
       "      <td>9196.46</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          features       beta                odds        SE  \\\n",
       "0                   intercept_term  11.825976  136759.05228577272  1.028866   \n",
       "1             product_data_storage  -0.000265  0.9997354906118859  0.000075   \n",
       "2                       csat_score  -1.114814  0.3279764392254565  0.101158   \n",
       "3                  articles_viewed  -0.042609   0.958286007085567  0.040354   \n",
       "4  smartphone_notifications_viewed   0.388169  1.4742782682460098  0.136885   \n",
       "5         marketing_emails_clicked  -0.037101  0.9635785556193255  0.025565   \n",
       "6          social_media_ads_viewed  -0.391455  0.6760726937037167  0.150134   \n",
       "7         minutes_customer_support   0.089820   1.093977080726584  0.015786   \n",
       "8                    months_active  -0.275176  0.7594381755978886  0.041341   \n",
       "\n",
       "   Walds Z  LikelihoodRatio-t  p_value_LR  \n",
       "0   11.494              34.02       0.000  \n",
       "1   -3.548            9054.76       0.000  \n",
       "2  -11.021               6.07       0.014  \n",
       "3   -1.056              12.16       0.000  \n",
       "4    2.836              60.53       0.000  \n",
       "5   -1.451              10.45       0.001  \n",
       "6   -2.607              54.57       0.000  \n",
       "7    5.690             249.79       0.000  \n",
       "8   -6.656            9196.46       0.000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hence, we see that all features are statistically significant and contribute to increase in Log-Likelihood. Considering Walds-Z and Likelihood Ratio-t stats together provides evidence for significances of effect and increase in Loglikelihood at the same time (See that few attributes have lower Z, but high T).\n",
    "\n",
    "\n",
    "* Likelihood-Ratio test can be used to decide when we want to include new features or interaction terms, and Walds Z can provide evidence of significance of coefficient to predict log-odds and interpret magnitude and direction of effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
