{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic GD vs Batch GD vs Mini-Batch GD\n",
    "\n",
    "* In the previous post, we saw how to construct 2 Layer Neural Network using back-prop and parameter updates. In this post, we take a look at how Mini-batch Gradient Descent can be a better option.\n",
    "\n",
    "\n",
    "* In the previous NN, every epoch corresponded to one update of the weight vector. Hence, if number of iterations is 1000, the weight vector is updated 1000 times, by using the <b> complete dataset </b> to calculate cost function. Hence, we used Batch Gradient Descent in this post <a href=\"https://github.com/AdiVarma27/AdiVarma27.github.io/blob/master/_jupyter/2019-04-07-Neural%20Nets%20Math%20%26%20Implementation%20(2%20Layered%20Net%20with%20n-Hidden%20Units).ipynb\">Batch Gradient Descent 2-Layered NN</a>.\n",
    "\n",
    "\n",
    "* However, in case of large datasets, images, audio-video processing, using the complete Batch (or complete dataset) can be extremely compute intensive, hence, we need to use SGD or Mini-batch, to update weight vectors based on cost of randomly chosen subset of the data, reducing the computational complexity of our Neural Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Gradient Descent\n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "* In Batch Gradient descent, for every iteration, the whole dataset/ feature matrix is used to calculate the cost function, which is used to update the weight parameters. Hence, we loop through only number of iterations and not any sub-sample of the dataset.\n",
    "\n",
    "\n",
    "* As the number of weight updates are fewer, the computational expense is much lesser when compared to Stochastic Gradient Descent. Also, the lesser frequency of weight updates generally leads to a more stable gradient, easily finding minima.\n",
    "\n",
    "\n",
    "* For extemely large datasets, holding the complete dataset in memory can be expensive and parameter updates can be extremely slow to calculate.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "* In SGD, for every iteration and every single row vector/ feature vector is used to calculate the cost, and used for the weight update step. Hence, the cost function as number of iterations increase can be noisy or sharp, as it tends to fit to the current point.\n",
    "\n",
    "\n",
    "* As the number of iterations times the number of rows in dataset can be extremely large (looping twice), using SGD is not preferred for datasets over 1000 rows in general. The computational complexity can increase tremendously, which is not preferred. Also, Due to the noisy cost updates, the gradient may not settle down and make it hard to converge to a minima.\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "* In Mini-batch Gradient Descent, the dataset chosen for calculating loss and weight update step is essentially a sub-sample of the original feature matrix, and it strikes the balance between flexibility of Stochastic GD (looping over every iteration for every data-point) and the efficiency of Batch GD (Single for loop to iterate through number of iterations).\n",
    "\n",
    "\n",
    "* Hence, it allows to train the model in memory, while handling non-noisy weight updates; essentially leading to a stable convergence. Only additional hyperparameter is batch size, which can be optimized using cross validation techniques.\n",
    "\n",
    "\n",
    "* With all this said, tuning learning rate is extremely important for any of the optimization algorithms. Also, the nature of the dataset can sometime end up with SGD or just Batch GD to perform better than Mini-Batch GD, However, Mini-batch GD is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folllowing function getnextBatch, returns 32 row vectors from feature matrix X and label vector y\n",
    "def getnextBatch(X, y, batch_size = 32):\n",
    "    \n",
    "    for i in np.arange(0, X.shape[0], batch_size):\n",
    "        \n",
    "        yield (X[i:i+batch_size], y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Function \n",
    "\n",
    "* Function returns feature matrix and label vector with batch_size number of rows, and the sample is random in nature. Hence, it reduces the tendency to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following function getnextBatchRandom, returns 32 random row vectors from the feature matrix X and label vector y\n",
    "def getnextMiniBatchRandom(X, y, batch_size = 32):\n",
    "    \n",
    "    for i in np.arange(0, X.shape[0], batch_size):\n",
    "        frac_val = batch_size/X.shape[0]\n",
    "        df = pd.DataFrame(X)\n",
    "        df['y'] = y\n",
    "        df = df.sample(frac=frac_val)\n",
    "        X = np.array(df[[0,1]])\n",
    "        y = np.array(df['y'])\n",
    "\n",
    "        yield (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Neural Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to initialize weights\n",
    "def initialize_weights(X, seed=27):\n",
    "    np.random.seed(27)\n",
    "    w = np.random.rand(X.shape[0],1)\n",
    "    b = 0\n",
    "    return w, b\n",
    "\n",
    "# propagation step\n",
    "def forward_prop(X, w, b):\n",
    "    Z = np.dot(w.T, X.T) + b\n",
    "    A = sigmoid(Z)    \n",
    "    return Z, A\n",
    "\n",
    "# function to compute cost\n",
    "def compute_cost(A, y, m):\n",
    "    cost = -(1/m)*np.sum(y*np.log(A) + (1-y)*np.log(1-A))\n",
    "    return cost\n",
    "\n",
    "# backpropagation step\n",
    "def back_prop(X, A, y, m):\n",
    "    dw = (1/m)*np.dot(X.T, (A-y).T)\n",
    "    db = (1/m)*np.sum(A-y)\n",
    "    return dw, db\n",
    "\n",
    "# parameter update step\n",
    "def update_params(w,b,dw,db):\n",
    "    w = w - learning_rate*dw\n",
    "    b = b - learning_rate*db\n",
    "    return w, b\n",
    "\n",
    "# See post https://adivarma27.github.io/LogisticRegressionCost/# for detailed explanation\n",
    "def trainMiniBatchGD(X, y, w, b, max_iterations=1000, learning_rate=0.01, batch_size=32):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    learning_rate = learning_rate\n",
    "    max_iterations = max_iterations\n",
    "    \n",
    "    cost_list, w1_list, w2_list, b_list = [], [], [], []\n",
    "    \n",
    "    # iteration\n",
    "    for iteration in range(max_iterations):\n",
    "        \n",
    "        # for each Mini-Batch Step\n",
    "        for (X_batch, y_batch) in getnextBatch(X, y, batch_size=batch_size):\n",
    "\n",
    "            # linear-combination, activation step   \n",
    "            Z, A = forward_prop(X_batch, w, b)\n",
    "\n",
    "            # compute cost for each data-point in each iteration\n",
    "            cost = compute_cost(A, y_batch, m)\n",
    "\n",
    "            # back-prop\n",
    "            # derivative with respect to cost function\n",
    "            dw, db = back_prop(X_batch, A, y_batch, m)\n",
    "\n",
    "            # parameter update step\n",
    "            w, b = update_params(w,b,dw,db)\n",
    "\n",
    "        cost_list.append(cost)\n",
    "\n",
    "        w1_list.append(w[0])\n",
    "        w2_list.append(w[1])\n",
    "        b_list.append(b)\n",
    "\n",
    "    return cost_list, w1_list, w2_list, b_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters alpha, max_iterations\n",
    "learning_rate = 0.0005\n",
    "max_iterations = 100\n",
    "\n",
    "# Initialize weights\n",
    "w, b = initialize_weights(X_train, seed=10)\n",
    "\n",
    "cost_list_32, w1_list_32, w2_list_32, b_list = trainMiniBatchGD(X, y, w, b, \n",
    "                                                                max_iterations=max_iterations, batch_size=32)\n",
    "\n",
    "cost_list_batch, w1_list_batch, w2_list_batch, b_list = trainMiniBatchGD(X, y, w, b, \n",
    "                                                                     max_iterations=max_iterations, batch_size=X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Implementation\n",
    "\n",
    "* Now, let us load a slightly more complex dataset and use different optimizers in Keras module.\n",
    "\n",
    "### Iteration vs Epoch \n",
    "\n",
    "* Iteration is equivalent to parsing through the whole feature matrix once, in Batch Gradient Descent; whereas Epoch corresponds to one complete pass through <b> ALL </b> the dataset once. Our code above is in terms of iterations and not in terms of epochs. \n",
    "\n",
    "\n",
    "* In all deep-learning libraries, we can adjust the number of epochs and batch size based on number of data-points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc701b6b9b0>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Complex dataset\n",
    "\n",
    "# source: https://github.com/rvarun7777/Deep_Learning/blob/master/Neural%20Networks%20and%20Deep%20Learning/Week%203/Planar%20data%20classification%20with%20one%20hidden%20layer/planar_utils.py\n",
    "# function to load our dataset\n",
    "def load_planar_dataset():\n",
    "    np.random.seed(1)\n",
    "    m = 400 # number of examples\n",
    "    N = int(m/2) # number of points per class\n",
    "    D = 2 # dimensionality\n",
    "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
    "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "    a = 4 # maximum ray of the flower\n",
    "\n",
    "    for j in range(2):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        Y[ix] = j\n",
    "        \n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# take features into X and targets into y\n",
    "X, y = load_planar_dataset()\n",
    "\n",
    "plt.scatter(X[0,:], X[1,:], c=y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing keras\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.T, y.T, test_size = 0.2, random_state = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sequential class\n",
    "classifier = Sequential()\n",
    "\n",
    "# adding first dense layer with relu activation and 8 hidden units\n",
    "classifier.add(Dense(input_dim = 2, units = 8, kernel_initializer = 'uniform', activation='relu'))\n",
    "\n",
    "# adding second dense layer with relu activation and 4 hidden units\n",
    "classifier.add(Dense(units = 8, activation = 'relu', kernel_initializer = 'uniform'))\n",
    "\n",
    "# adding third dense layer with relu activation and 4 hidden units\n",
    "classifier.add(Dense(units = 4, activation = 'relu', kernel_initializer = 'uniform'))\n",
    "\n",
    "# adding final sigmoind activation dense layer with 1 unit\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer = 'uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling NN model using sgd, with batch size 32\n",
    "classifier.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "320/320 [==============================] - 1s 5ms/step - loss: 0.6928 - acc: 0.5125\n",
      "Epoch 2/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6920 - acc: 0.5312\n",
      "Epoch 3/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6915 - acc: 0.5312\n",
      "Epoch 4/100\n",
      "320/320 [==============================] - 0s 50us/step - loss: 0.6920 - acc: 0.5312\n",
      "Epoch 5/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 6/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6918 - acc: 0.5312\n",
      "Epoch 7/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 8/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6923 - acc: 0.5312\n",
      "Epoch 9/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 10/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 11/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6920 - acc: 0.5312\n",
      "Epoch 12/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 13/100\n",
      "320/320 [==============================] - 0s 49us/step - loss: 0.6925 - acc: 0.5312\n",
      "Epoch 14/100\n",
      "320/320 [==============================] - 0s 49us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 15/100\n",
      "320/320 [==============================] - 0s 51us/step - loss: 0.6924 - acc: 0.5312\n",
      "Epoch 16/100\n",
      "320/320 [==============================] - 0s 54us/step - loss: 0.6922 - acc: 0.5312\n",
      "Epoch 17/100\n",
      "320/320 [==============================] - 0s 51us/step - loss: 0.6920 - acc: 0.5312\n",
      "Epoch 18/100\n",
      "320/320 [==============================] - 0s 50us/step - loss: 0.6915 - acc: 0.5312\n",
      "Epoch 19/100\n",
      "320/320 [==============================] - 0s 51us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 20/100\n",
      "320/320 [==============================] - 0s 48us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 21/100\n",
      "320/320 [==============================] - 0s 49us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 22/100\n",
      "320/320 [==============================] - 0s 52us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 23/100\n",
      "320/320 [==============================] - 0s 50us/step - loss: 0.6918 - acc: 0.5312\n",
      "Epoch 24/100\n",
      "320/320 [==============================] - 0s 53us/step - loss: 0.6918 - acc: 0.5312\n",
      "Epoch 25/100\n",
      "320/320 [==============================] - 0s 53us/step - loss: 0.6922 - acc: 0.5312\n",
      "Epoch 26/100\n",
      "320/320 [==============================] - 0s 49us/step - loss: 0.6920 - acc: 0.5312\n",
      "Epoch 27/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 28/100\n",
      "320/320 [==============================] - 0s 49us/step - loss: 0.6923 - acc: 0.5312\n",
      "Epoch 29/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6926 - acc: 0.5312\n",
      "Epoch 30/100\n",
      "320/320 [==============================] - 0s 56us/step - loss: 0.6925 - acc: 0.5312\n",
      "Epoch 31/100\n",
      "320/320 [==============================] - 0s 54us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 32/100\n",
      "320/320 [==============================] - 0s 53us/step - loss: 0.6927 - acc: 0.5312\n",
      "Epoch 33/100\n",
      "320/320 [==============================] - 0s 51us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 34/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 35/100\n",
      "320/320 [==============================] - 0s 56us/step - loss: 0.6926 - acc: 0.5312\n",
      "Epoch 36/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 37/100\n",
      "320/320 [==============================] - 0s 51us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 38/100\n",
      "320/320 [==============================] - 0s 48us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 39/100\n",
      "320/320 [==============================] - 0s 52us/step - loss: 0.6922 - acc: 0.5312\n",
      "Epoch 40/100\n",
      "320/320 [==============================] - 0s 52us/step - loss: 0.6933 - acc: 0.5312\n",
      "Epoch 41/100\n",
      "320/320 [==============================] - 0s 50us/step - loss: 0.6915 - acc: 0.5312\n",
      "Epoch 42/100\n",
      "320/320 [==============================] - 0s 50us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 43/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6925 - acc: 0.5312\n",
      "Epoch 44/100\n",
      "320/320 [==============================] - 0s 51us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 45/100\n",
      "320/320 [==============================] - 0s 48us/step - loss: 0.6925 - acc: 0.5312\n",
      "Epoch 46/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 47/100\n",
      "320/320 [==============================] - 0s 48us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 48/100\n",
      "320/320 [==============================] - 0s 56us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 49/100\n",
      "320/320 [==============================] - 0s 50us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 50/100\n",
      "320/320 [==============================] - 0s 50us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 51/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6921 - acc: 0.5312\n",
      "Epoch 52/100\n",
      "320/320 [==============================] - 0s 49us/step - loss: 0.6922 - acc: 0.5312\n",
      "Epoch 53/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 54/100\n",
      "320/320 [==============================] - 0s 50us/step - loss: 0.6915 - acc: 0.5312\n",
      "Epoch 55/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 56/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6915 - acc: 0.5312\n",
      "Epoch 57/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 58/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 59/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 60/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 61/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 62/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 63/100\n",
      "320/320 [==============================] - 0s 48us/step - loss: 0.6920 - acc: 0.5312\n",
      "Epoch 64/100\n",
      "320/320 [==============================] - 0s 48us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 65/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 66/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 67/100\n",
      "320/320 [==============================] - 0s 48us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 68/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6918 - acc: 0.5312\n",
      "Epoch 69/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6921 - acc: 0.5312\n",
      "Epoch 70/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 71/100\n",
      "320/320 [==============================] - 0s 44us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 72/100\n",
      "320/320 [==============================] - 0s 44us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 73/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 74/100\n",
      "320/320 [==============================] - 0s 51us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 75/100\n",
      "320/320 [==============================] - 0s 50us/step - loss: 0.6920 - acc: 0.5312\n",
      "Epoch 76/100\n",
      "320/320 [==============================] - 0s 48us/step - loss: 0.6921 - acc: 0.5312\n",
      "Epoch 77/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6918 - acc: 0.5312\n",
      "Epoch 78/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 79/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6924 - acc: 0.5312\n",
      "Epoch 80/100\n",
      "320/320 [==============================] - 0s 48us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 81/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 82/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6918 - acc: 0.5312\n",
      "Epoch 83/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 84/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6922 - acc: 0.5312\n",
      "Epoch 85/100\n",
      "320/320 [==============================] - 0s 49us/step - loss: 0.6924 - acc: 0.5312\n",
      "Epoch 86/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6924 - acc: 0.5312\n",
      "Epoch 87/100\n",
      "320/320 [==============================] - 0s 44us/step - loss: 0.6918 - acc: 0.5312\n",
      "Epoch 88/100\n",
      "320/320 [==============================] - 0s 44us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 89/100\n",
      "320/320 [==============================] - 0s 43us/step - loss: 0.6915 - acc: 0.5312\n",
      "Epoch 90/100\n",
      "320/320 [==============================] - 0s 43us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 91/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6922 - acc: 0.5312\n",
      "Epoch 92/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6921 - acc: 0.5312\n",
      "Epoch 93/100\n",
      "320/320 [==============================] - 0s 47us/step - loss: 0.6921 - acc: 0.5312\n",
      "Epoch 94/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 95/100\n",
      "320/320 [==============================] - 0s 46us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 96/100\n",
      "320/320 [==============================] - 0s 44us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 97/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 98/100\n",
      "320/320 [==============================] - 0s 44us/step - loss: 0.6921 - acc: 0.5312\n",
      "Epoch 99/100\n",
      "320/320 [==============================] - 0s 45us/step - loss: 0.6921 - acc: 0.5312\n",
      "Epoch 100/100\n",
      "320/320 [==============================] - 0s 49us/step - loss: 0.6918 - acc: 0.5312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6f0ed1630>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting ytraining data with batch size and number of epochs\n",
    "classifier.fit(X_train, y_train, batch_size=32, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# changing probabilities to 1 or 0\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 10],\n",
       "       [ 9, 21]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV across parameters\n",
    "\n",
    "* We observe that the accuracy score is just 0.375 inspite of having 4 layers, Hence, we use GridSearchCV to find best parameters. \n",
    "\n",
    "\n",
    "* We adjust across bacth_size and number of epochs for grid seach CV, and find out that the classifier find it hard to optimize using SGD with any batch size. We can manually tweak the learning rate and adjust to it to converge.\n",
    "\n",
    "\n",
    "* WE see that 'adam' optimization is much more efficient and finds the minima easily. Hence, we solve our problem using Adam optimization. In the next pose, lets take a look at RMSProp and Adam in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf():\n",
    "    # importing sequential class\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # adding first dense layer with relu activation and 8 hidden units\n",
    "    classifier.add(Dense(input_dim = 2, units = 8, kernel_initializer = 'uniform', activation='relu'))\n",
    "\n",
    "    # adding second dense layer with relu activation and 4 hidden units\n",
    "    classifier.add(Dense(units = 8, activation = 'relu', kernel_initializer = 'uniform'))\n",
    "\n",
    "    # adding third dense layer with relu activation and 4 hidden units\n",
    "    classifier.add(Dense(units = 4, activation = 'relu', kernel_initializer = 'uniform'))\n",
    "\n",
    "    # adding final sigmoind activation dense layer with 1 unit\n",
    "    classifier.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer = 'uniform'))\n",
    "    \n",
    "    # compiling NN model using sgd, with batch size 32\n",
    "    classifier.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(build_fn = build_clf)\n",
    "\n",
    "parameters = {\"batch_size\": [8, 64],\n",
    "              \"epochs\": [100, 200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=classifier, param_grid=parameters, scoring='accuracy', cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6932 - acc: 0.5188\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6933 - acc: 0.5062\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6932 - acc: 0.5188\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6930 - acc: 0.5188\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6932 - acc: 0.4688\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6930 - acc: 0.5188\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6930 - acc: 0.5188\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6929 - acc: 0.5188\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6930 - acc: 0.5188\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6928 - acc: 0.5188\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6928 - acc: 0.5188\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6928 - acc: 0.5188\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6928 - acc: 0.5188\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 0s 250us/step - loss: 0.6928 - acc: 0.5188\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6928 - acc: 0.5188\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 0s 237us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 0s 236us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 0s 240us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 0s 222us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.6932 - acc: 0.5188\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 0s 246us/step - loss: 0.6928 - acc: 0.5437\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.6924 - acc: 0.5437\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6922 - acc: 0.5437\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6919 - acc: 0.5437\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6917 - acc: 0.5437\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6914 - acc: 0.5437\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6912 - acc: 0.5437\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6911 - acc: 0.5437\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.6910 - acc: 0.5437\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6908 - acc: 0.5437\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 0s 222us/step - loss: 0.6906 - acc: 0.5437\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6905 - acc: 0.5437\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6904 - acc: 0.5437\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6903 - acc: 0.5437\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.6903 - acc: 0.5437\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6903 - acc: 0.5437\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.6901 - acc: 0.5437\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6901 - acc: 0.5437\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6900 - acc: 0.5437\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6900 - acc: 0.5437\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.6899 - acc: 0.5437\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.6899 - acc: 0.5437\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6898 - acc: 0.5437\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.6898 - acc: 0.5437\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6898 - acc: 0.5437\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6897 - acc: 0.5437\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.6897 - acc: 0.5437\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.6897 - acc: 0.5437\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6897 - acc: 0.5437\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6897 - acc: 0.5437\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 0s 234us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 0s 243us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6895 - acc: 0.5437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 0s 246us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 0s 274us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 0s 255us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 0s 238us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 1/200\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6933 - acc: 0.4812\n",
      "Epoch 2/200\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6932 - acc: 0.4938\n",
      "Epoch 3/200\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.6931 - acc: 0.5188\n",
      "Epoch 4/200\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6931 - acc: 0.5188\n",
      "Epoch 5/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6930 - acc: 0.5188\n",
      "Epoch 6/200\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6930 - acc: 0.5188\n",
      "Epoch 7/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6930 - acc: 0.5188\n",
      "Epoch 8/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6929 - acc: 0.5188\n",
      "Epoch 9/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6928 - acc: 0.5188\n",
      "Epoch 10/200\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6928 - acc: 0.5188\n",
      "Epoch 11/200\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6929 - acc: 0.5188\n",
      "Epoch 12/200\n",
      "160/160 [==============================] - 0s 243us/step - loss: 0.6928 - acc: 0.5188\n",
      "Epoch 13/200\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6928 - acc: 0.5188\n",
      "Epoch 14/200\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.6928 - acc: 0.5188\n",
      "Epoch 15/200\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 16/200\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 17/200\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 18/200\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 19/200\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 20/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 21/200\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 22/200\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 23/200\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 24/200\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 25/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 26/200\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 27/200\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 28/200\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 29/200\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 30/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 31/200\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 32/200\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 33/200\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 34/200\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 35/200\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 36/200\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 37/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 38/200\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 39/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 40/200\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 41/200\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 42/200\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 43/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 44/200\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 45/200\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 46/200\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 47/200\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 48/200\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 49/200\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 50/200\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.6928 - acc: 0.5188\n",
      "Epoch 51/200\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 52/200\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 53/200\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 54/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 55/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 56/200\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 57/200\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 58/200\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 59/200\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 60/200\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 61/200\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 62/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 63/200\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 64/200\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 65/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 66/200\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 67/200\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 68/200\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 69/200\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 70/200\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 71/200\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 72/200\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 73/200\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 74/200\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 75/200\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 76/200\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 77/200\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 78/200\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 79/200\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 80/200\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 81/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 82/200\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 83/200\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 84/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 85/200\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 86/200\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 87/200\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 88/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 89/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 90/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 91/200\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 92/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 93/200\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 94/200\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 95/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 96/200\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 97/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 98/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 99/200\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 100/200\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 101/200\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 102/200\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 103/200\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 104/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 105/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 106/200\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 107/200\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 108/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 109/200\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 110/200\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 111/200\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 112/200\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 113/200\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 114/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 115/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 116/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 117/200\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 118/200\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 119/200\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 120/200\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 121/200\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 122/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 123/200\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 124/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 125/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 126/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 127/200\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 128/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 129/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 130/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 215us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 131/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 132/200\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 133/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 134/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 135/200\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 136/200\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 137/200\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 138/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 139/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 140/200\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 141/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 142/200\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 143/200\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 144/200\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 145/200\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 146/200\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 147/200\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 148/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 149/200\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 150/200\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 151/200\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 152/200\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 153/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 154/200\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 155/200\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 156/200\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 157/200\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 158/200\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 159/200\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 160/200\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 161/200\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 162/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 163/200\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 164/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 165/200\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 166/200\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 167/200\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 168/200\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 169/200\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 170/200\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 171/200\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 172/200\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 173/200\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 174/200\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 175/200\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 176/200\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 177/200\n",
      "160/160 [==============================] - 0s 257us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 178/200\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 179/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 180/200\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 181/200\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 182/200\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 183/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 184/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 185/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 186/200\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 187/200\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 188/200\n",
      "160/160 [==============================] - 0s 222us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 189/200\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 190/200\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 191/200\n",
      "160/160 [==============================] - 0s 243us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 192/200\n",
      "160/160 [==============================] - 0s 237us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 193/200\n",
      "160/160 [==============================] - 0s 233us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 194/200\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 195/200\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6925 - acc: 0.5188\n",
      "Epoch 196/200\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 197/200\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 198/200\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6927 - acc: 0.5188\n",
      "Epoch 199/200\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 200/200\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6926 - acc: 0.5188\n",
      "Epoch 1/200\n",
      "160/160 [==============================] - 4s 22ms/step - loss: 0.6931 - acc: 0.5312\n",
      "Epoch 2/200\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.6927 - acc: 0.5437\n",
      "Epoch 3/200\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6925 - acc: 0.5437\n",
      "Epoch 4/200\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6922 - acc: 0.5437\n",
      "Epoch 5/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6919 - acc: 0.5437\n",
      "Epoch 6/200\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6916 - acc: 0.5437\n",
      "Epoch 7/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6915 - acc: 0.5437\n",
      "Epoch 8/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6913 - acc: 0.5437\n",
      "Epoch 9/200\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6910 - acc: 0.5437\n",
      "Epoch 10/200\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6910 - acc: 0.5437\n",
      "Epoch 11/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6908 - acc: 0.5437\n",
      "Epoch 12/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6907 - acc: 0.5437\n",
      "Epoch 13/200\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6905 - acc: 0.5437\n",
      "Epoch 14/200\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6905 - acc: 0.5437\n",
      "Epoch 15/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6903 - acc: 0.5437\n",
      "Epoch 16/200\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6902 - acc: 0.5437\n",
      "Epoch 17/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6902 - acc: 0.5437\n",
      "Epoch 18/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6901 - acc: 0.5437\n",
      "Epoch 19/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6901 - acc: 0.5437\n",
      "Epoch 20/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6900 - acc: 0.5437\n",
      "Epoch 21/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6901 - acc: 0.5437\n",
      "Epoch 22/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6899 - acc: 0.5437\n",
      "Epoch 23/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6899 - acc: 0.5437\n",
      "Epoch 24/200\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6898 - acc: 0.5437\n",
      "Epoch 25/200\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6899 - acc: 0.5437\n",
      "Epoch 26/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6897 - acc: 0.5437\n",
      "Epoch 27/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6898 - acc: 0.5437\n",
      "Epoch 28/200\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6898 - acc: 0.5437\n",
      "Epoch 29/200\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6897 - acc: 0.5437\n",
      "Epoch 30/200\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6897 - acc: 0.5437\n",
      "Epoch 31/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6897 - acc: 0.5437\n",
      "Epoch 32/200\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 33/200\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 34/200\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 35/200\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 36/200\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 37/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 38/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 39/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 40/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 41/200\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 42/200\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 43/200\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 44/200\n",
      "160/160 [==============================] - 0s 222us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 45/200\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 46/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 47/200\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 48/200\n",
      "160/160 [==============================] - 0s 222us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 49/200\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 50/200\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 51/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 52/200\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 53/200\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 54/200\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 55/200\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 56/200\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 57/200\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 58/200\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 59/200\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 60/200\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 61/200\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 62/200\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 63/200\n",
      "160/160 [==============================] - 0s 222us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 64/200\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 65/200\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 66/200\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 67/200\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 68/200\n",
      "160/160 [==============================] - 0s 233us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 69/200\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 70/200\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 71/200\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 72/200\n",
      "160/160 [==============================] - 0s 222us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 73/200\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 74/200\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 75/200\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 76/200\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 77/200\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 78/200\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 79/200\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 80/200\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 81/200\n",
      "160/160 [==============================] - 0s 227us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 82/200\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 83/200\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 84/200\n",
      "160/160 [==============================] - 0s 240us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 85/200\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 86/200\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 87/200\n",
      "160/160 [==============================] - 0s 238us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 88/200\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 89/200\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 90/200\n",
      "160/160 [==============================] - 0s 240us/step - loss: 0.6893 - acc: 0.5437\n",
      "Epoch 91/200\n",
      "160/160 [==============================] - 0s 233us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 92/200\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 93/200\n",
      "160/160 [==============================] - 0s 238us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 94/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 227us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 95/200\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 96/200\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 97/200\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 98/200\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 99/200\n",
      "160/160 [==============================] - 0s 234us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 100/200\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 101/200\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.6896 - acc: 0.5437\n",
      "Epoch 102/200\n",
      "160/160 [==============================] - 0s 239us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 103/200\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 104/200\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 105/200\n",
      "160/160 [==============================] - 0s 233us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 106/200\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 107/200\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 108/200\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 109/200\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 110/200\n",
      "160/160 [==============================] - 0s 228us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 111/200\n",
      "160/160 [==============================] - 0s 236us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 112/200\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 113/200\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 114/200\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 115/200\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 116/200\n",
      "160/160 [==============================] - 0s 239us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 117/200\n",
      "160/160 [==============================] - 0s 234us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 118/200\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 119/200\n",
      "160/160 [==============================] - 0s 252us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 120/200\n",
      "160/160 [==============================] - 0s 239us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 121/200\n",
      "160/160 [==============================] - 0s 237us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 122/200\n",
      "160/160 [==============================] - 0s 234us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 123/200\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 124/200\n",
      "160/160 [==============================] - 0s 238us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 125/200\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 126/200\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 127/200\n",
      "160/160 [==============================] - 0s 239us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 128/200\n",
      "160/160 [==============================] - 0s 239us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 129/200\n",
      "160/160 [==============================] - 0s 236us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 130/200\n",
      "160/160 [==============================] - 0s 239us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 131/200\n",
      "160/160 [==============================] - 0s 245us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 132/200\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 133/200\n",
      "160/160 [==============================] - 0s 232us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 134/200\n",
      "160/160 [==============================] - 0s 233us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 135/200\n",
      "160/160 [==============================] - 0s 234us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 136/200\n",
      "160/160 [==============================] - 0s 236us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 137/200\n",
      "160/160 [==============================] - 0s 236us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 138/200\n",
      "160/160 [==============================] - 0s 241us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 139/200\n",
      "160/160 [==============================] - 0s 245us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 140/200\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 141/200\n",
      "160/160 [==============================] - 0s 245us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 142/200\n",
      "160/160 [==============================] - 0s 245us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 143/200\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 144/200\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 145/200\n",
      "160/160 [==============================] - 0s 247us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 146/200\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 147/200\n",
      "160/160 [==============================] - 0s 243us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 148/200\n",
      "160/160 [==============================] - 0s 238us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 149/200\n",
      "160/160 [==============================] - 0s 240us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 150/200\n",
      "160/160 [==============================] - 0s 239us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 151/200\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 152/200\n",
      "160/160 [==============================] - 0s 250us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 153/200\n",
      "160/160 [==============================] - 0s 265us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 154/200\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 155/200\n",
      "160/160 [==============================] - 0s 248us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 156/200\n",
      "160/160 [==============================] - 0s 243us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 157/200\n",
      "160/160 [==============================] - 0s 243us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 158/200\n",
      "160/160 [==============================] - 0s 247us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 159/200\n",
      "160/160 [==============================] - 0s 245us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 160/200\n",
      "160/160 [==============================] - 0s 245us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 161/200\n",
      "160/160 [==============================] - 0s 246us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 162/200\n",
      "160/160 [==============================] - 0s 245us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 163/200\n",
      "160/160 [==============================] - 0s 250us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 164/200\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 165/200\n",
      "160/160 [==============================] - 0s 240us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 166/200\n",
      "160/160 [==============================] - 0s 239us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 167/200\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 168/200\n",
      "160/160 [==============================] - 0s 243us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 169/200\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 170/200\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 171/200\n",
      "160/160 [==============================] - 0s 250us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 172/200\n",
      "160/160 [==============================] - 0s 239us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 173/200\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 174/200\n",
      "160/160 [==============================] - 0s 241us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 175/200\n",
      "160/160 [==============================] - 0s 250us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 250us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 177/200\n",
      "160/160 [==============================] - 0s 247us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 178/200\n",
      "160/160 [==============================] - 0s 249us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 179/200\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 180/200\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 181/200\n",
      "160/160 [==============================] - 0s 252us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 182/200\n",
      "160/160 [==============================] - 0s 249us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 183/200\n",
      "160/160 [==============================] - 0s 247us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 184/200\n",
      "160/160 [==============================] - 0s 239us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 185/200\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 186/200\n",
      "160/160 [==============================] - 0s 251us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 187/200\n",
      "160/160 [==============================] - 0s 247us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 188/200\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 189/200\n",
      "160/160 [==============================] - 0s 247us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 190/200\n",
      "160/160 [==============================] - 0s 246us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 191/200\n",
      "160/160 [==============================] - 0s 245us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 192/200\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 193/200\n",
      "160/160 [==============================] - 0s 248us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 194/200\n",
      "160/160 [==============================] - 0s 240us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 195/200\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 196/200\n",
      "160/160 [==============================] - 0s 250us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 197/200\n",
      "160/160 [==============================] - 0s 262us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 198/200\n",
      "160/160 [==============================] - 0s 252us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 199/200\n",
      "160/160 [==============================] - 0s 252us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 200/200\n",
      "160/160 [==============================] - 0s 252us/step - loss: 0.6895 - acc: 0.5437\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 4s 23ms/step - loss: 0.6932 - acc: 0.4188\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6932 - acc: 0.5187\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6932 - acc: 0.4312\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 0s 49us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 0s 48us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 0s 49us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 0s 50us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 0s 50us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6931 - acc: 0.5438\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6930 - acc: 0.5438\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6930 - acc: 0.5438\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6929 - acc: 0.5438\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6929 - acc: 0.5438\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6928 - acc: 0.5438\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6928 - acc: 0.5438\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6927 - acc: 0.5438\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6926 - acc: 0.5438\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6926 - acc: 0.5438\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6926 - acc: 0.5438\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6925 - acc: 0.5438\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6925 - acc: 0.5438\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6924 - acc: 0.5438\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6924 - acc: 0.5438\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6923 - acc: 0.5438\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6923 - acc: 0.5438\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6922 - acc: 0.5438\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6922 - acc: 0.5438\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6921 - acc: 0.5438\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6921 - acc: 0.5438\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6920 - acc: 0.5438\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6920 - acc: 0.5438\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6920 - acc: 0.5438\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6919 - acc: 0.5438\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6919 - acc: 0.5438\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6919 - acc: 0.5438\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6918 - acc: 0.5438\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6918 - acc: 0.5438\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6917 - acc: 0.5438\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6917 - acc: 0.5438\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6917 - acc: 0.5438\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6917 - acc: 0.5438\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6916 - acc: 0.5438\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6916 - acc: 0.5438\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6916 - acc: 0.5438\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6915 - acc: 0.5438\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6915 - acc: 0.5438\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6915 - acc: 0.5438\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6915 - acc: 0.5438\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 38us/step - loss: 0.6915 - acc: 0.5438\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6914 - acc: 0.5438\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6914 - acc: 0.5438\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6914 - acc: 0.5438\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6913 - acc: 0.5438\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6913 - acc: 0.5438\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6913 - acc: 0.5438\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6913 - acc: 0.5438\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6912 - acc: 0.5438\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6912 - acc: 0.5438\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6912 - acc: 0.5438\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6912 - acc: 0.5438\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6911 - acc: 0.5438\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6911 - acc: 0.5438\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6911 - acc: 0.5438\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6910 - acc: 0.5438\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6910 - acc: 0.5438\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6910 - acc: 0.5438\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6909 - acc: 0.5438\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6910 - acc: 0.5438\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6909 - acc: 0.5438\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6908 - acc: 0.5438\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6908 - acc: 0.5438\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6908 - acc: 0.5438\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6908 - acc: 0.5438\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6908 - acc: 0.5438\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6907 - acc: 0.5438\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6907 - acc: 0.5438\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6907 - acc: 0.5438\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6905 - acc: 0.5438\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6905 - acc: 0.5438\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6903 - acc: 0.5438\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6903 - acc: 0.5438\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6903 - acc: 0.5438\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6903 - acc: 0.5438\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6903 - acc: 0.5438\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 1/200\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6932 - acc: 0.4813\n",
      "Epoch 2/200\n",
      "160/160 [==============================] - 0s 49us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 3/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 4/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 5/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 6/200\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 7/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6932 - acc: 0.5187\n",
      "Epoch 8/200\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 9/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 10/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 11/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 12/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 13/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 14/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 15/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 16/200\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 17/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 18/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 19/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 20/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 21/200\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 22/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6931 - acc: 0.5187\n",
      "Epoch 23/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 24/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 25/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 26/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 27/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 28/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 29/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 30/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 31/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 32/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6930 - acc: 0.5187\n",
      "Epoch 33/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 34/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 35/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 36/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 37/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 38/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 39/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 40/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 41/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 42/200\n",
      "160/160 [==============================] - 0s 34us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 43/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 44/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6929 - acc: 0.5187\n",
      "Epoch 45/200\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 46/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 47/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 48/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 49/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 50/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 51/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 52/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 53/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 54/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 55/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 56/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 57/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 58/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 59/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 60/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 61/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 62/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 63/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6928 - acc: 0.5187\n",
      "Epoch 64/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 65/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 66/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 67/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 68/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 69/200\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 70/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 71/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 72/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 73/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 74/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 75/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 76/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 77/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 78/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 79/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 80/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 81/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 82/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 83/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 84/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 85/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 86/200\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 87/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 88/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 89/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 90/200\n",
      "160/160 [==============================] - 0s 48us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 91/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 92/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 93/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 94/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 95/200\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 96/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 97/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 98/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6927 - acc: 0.5187\n",
      "Epoch 99/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 100/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 101/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 102/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 103/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 104/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 105/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 106/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 107/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 108/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 36us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 109/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 110/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 111/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 112/200\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 113/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 114/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 115/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 116/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 117/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 118/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 119/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 120/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 121/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 122/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 123/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 124/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 125/200\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 126/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 127/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 128/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 129/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 130/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 131/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 132/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 133/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 134/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 135/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 136/200\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 137/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 138/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 139/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 140/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 141/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 142/200\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 143/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 144/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 145/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 146/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 147/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 148/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 149/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 150/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 151/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 152/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 153/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 154/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 155/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 156/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 157/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 158/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 159/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 160/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 161/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 162/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 163/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 164/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 165/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 166/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 167/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 168/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 169/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 170/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 171/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 172/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 173/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 174/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 175/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 176/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 177/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 178/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 179/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 180/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 181/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 182/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 183/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6926 - acc: 0.5187\n",
      "Epoch 184/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 185/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 186/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 187/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 188/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 189/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 190/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 191/200\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 192/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 193/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 194/200\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 195/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 196/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 197/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 198/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 199/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 200/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6925 - acc: 0.5187\n",
      "Epoch 1/200\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6932 - acc: 0.4125\n",
      "Epoch 2/200\n",
      "160/160 [==============================] - 0s 49us/step - loss: 0.6931 - acc: 0.5438\n",
      "Epoch 3/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6930 - acc: 0.5438\n",
      "Epoch 4/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6929 - acc: 0.5438\n",
      "Epoch 5/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6929 - acc: 0.5438\n",
      "Epoch 6/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6929 - acc: 0.5438\n",
      "Epoch 7/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6928 - acc: 0.5438\n",
      "Epoch 8/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6928 - acc: 0.5438\n",
      "Epoch 9/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6927 - acc: 0.5438\n",
      "Epoch 10/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6926 - acc: 0.5438\n",
      "Epoch 11/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6926 - acc: 0.5438\n",
      "Epoch 12/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6926 - acc: 0.5438\n",
      "Epoch 13/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6925 - acc: 0.5438\n",
      "Epoch 14/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6925 - acc: 0.5438\n",
      "Epoch 15/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6924 - acc: 0.5438\n",
      "Epoch 16/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6924 - acc: 0.5438\n",
      "Epoch 17/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6923 - acc: 0.5438\n",
      "Epoch 18/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6923 - acc: 0.5438\n",
      "Epoch 19/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6922 - acc: 0.5438\n",
      "Epoch 20/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6922 - acc: 0.5438\n",
      "Epoch 21/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6922 - acc: 0.5438\n",
      "Epoch 22/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6921 - acc: 0.5438\n",
      "Epoch 23/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6921 - acc: 0.5438\n",
      "Epoch 24/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6920 - acc: 0.5438\n",
      "Epoch 25/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6920 - acc: 0.5438\n",
      "Epoch 26/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6919 - acc: 0.5438\n",
      "Epoch 27/200\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6919 - acc: 0.5438\n",
      "Epoch 28/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6919 - acc: 0.5438\n",
      "Epoch 29/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6919 - acc: 0.5438\n",
      "Epoch 30/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6918 - acc: 0.5438\n",
      "Epoch 31/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6918 - acc: 0.5438\n",
      "Epoch 32/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6917 - acc: 0.5438\n",
      "Epoch 33/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6917 - acc: 0.5438\n",
      "Epoch 34/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6916 - acc: 0.5438\n",
      "Epoch 35/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6916 - acc: 0.5438\n",
      "Epoch 36/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6916 - acc: 0.5438\n",
      "Epoch 37/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6915 - acc: 0.5438\n",
      "Epoch 38/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6915 - acc: 0.5438\n",
      "Epoch 39/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6914 - acc: 0.5438\n",
      "Epoch 40/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6914 - acc: 0.5438\n",
      "Epoch 41/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6914 - acc: 0.5438\n",
      "Epoch 42/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6914 - acc: 0.5438\n",
      "Epoch 43/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6914 - acc: 0.5438\n",
      "Epoch 44/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6913 - acc: 0.5438\n",
      "Epoch 45/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6913 - acc: 0.5438\n",
      "Epoch 46/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6913 - acc: 0.5438\n",
      "Epoch 47/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6912 - acc: 0.5438\n",
      "Epoch 48/200\n",
      "160/160 [==============================] - 0s 47us/step - loss: 0.6912 - acc: 0.5438\n",
      "Epoch 49/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6911 - acc: 0.5438\n",
      "Epoch 50/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6911 - acc: 0.5438\n",
      "Epoch 51/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6911 - acc: 0.5438\n",
      "Epoch 52/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6911 - acc: 0.5438\n",
      "Epoch 53/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6911 - acc: 0.5438\n",
      "Epoch 54/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6910 - acc: 0.5438\n",
      "Epoch 55/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6910 - acc: 0.5438\n",
      "Epoch 56/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6910 - acc: 0.5438\n",
      "Epoch 57/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6910 - acc: 0.5438\n",
      "Epoch 58/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6910 - acc: 0.5438\n",
      "Epoch 59/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6909 - acc: 0.5438\n",
      "Epoch 60/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6909 - acc: 0.5438\n",
      "Epoch 61/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6909 - acc: 0.5438\n",
      "Epoch 62/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6909 - acc: 0.5438\n",
      "Epoch 63/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6908 - acc: 0.5438\n",
      "Epoch 64/200\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6908 - acc: 0.5438\n",
      "Epoch 65/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6908 - acc: 0.5438\n",
      "Epoch 66/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6908 - acc: 0.5438\n",
      "Epoch 67/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6907 - acc: 0.5438\n",
      "Epoch 68/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6907 - acc: 0.5438\n",
      "Epoch 69/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6907 - acc: 0.5438\n",
      "Epoch 70/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6907 - acc: 0.5438\n",
      "Epoch 71/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6907 - acc: 0.5438\n",
      "Epoch 72/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 73/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 41us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 74/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 75/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 76/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 77/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6906 - acc: 0.5438\n",
      "Epoch 78/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6905 - acc: 0.5438\n",
      "Epoch 79/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6905 - acc: 0.5438\n",
      "Epoch 80/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6905 - acc: 0.5438\n",
      "Epoch 81/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 82/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 83/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6905 - acc: 0.5438\n",
      "Epoch 84/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 85/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 86/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 87/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 88/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6904 - acc: 0.5438\n",
      "Epoch 89/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6903 - acc: 0.5438\n",
      "Epoch 90/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6903 - acc: 0.5438\n",
      "Epoch 91/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6903 - acc: 0.5438\n",
      "Epoch 92/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6903 - acc: 0.5438\n",
      "Epoch 93/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6903 - acc: 0.5438\n",
      "Epoch 94/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6903 - acc: 0.5438\n",
      "Epoch 95/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 96/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 97/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 98/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 99/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 100/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 101/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 102/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6902 - acc: 0.5438\n",
      "Epoch 103/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6901 - acc: 0.5438\n",
      "Epoch 104/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6901 - acc: 0.5438\n",
      "Epoch 105/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6901 - acc: 0.5438\n",
      "Epoch 106/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6901 - acc: 0.5438\n",
      "Epoch 107/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6901 - acc: 0.5438\n",
      "Epoch 108/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6901 - acc: 0.5438\n",
      "Epoch 109/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6901 - acc: 0.5438\n",
      "Epoch 110/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6901 - acc: 0.5438\n",
      "Epoch 111/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 112/200\n",
      "160/160 [==============================] - 0s 35us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 113/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 114/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 115/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 116/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 117/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 118/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 119/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 120/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 121/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 122/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 123/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 124/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 125/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 126/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6900 - acc: 0.5438\n",
      "Epoch 127/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6899 - acc: 0.5438\n",
      "Epoch 128/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6899 - acc: 0.5438\n",
      "Epoch 129/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6899 - acc: 0.5438\n",
      "Epoch 130/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6899 - acc: 0.5438\n",
      "Epoch 131/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6899 - acc: 0.5438\n",
      "Epoch 132/200\n",
      "160/160 [==============================] - 0s 36us/step - loss: 0.6899 - acc: 0.5438\n",
      "Epoch 133/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6899 - acc: 0.5438\n",
      "Epoch 134/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6899 - acc: 0.5438\n",
      "Epoch 135/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6899 - acc: 0.5438\n",
      "Epoch 136/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6898 - acc: 0.5438\n",
      "Epoch 137/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6899 - acc: 0.5438\n",
      "Epoch 138/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6898 - acc: 0.5438\n",
      "Epoch 139/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6898 - acc: 0.5438\n",
      "Epoch 140/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6898 - acc: 0.5438\n",
      "Epoch 141/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6898 - acc: 0.5438\n",
      "Epoch 142/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6898 - acc: 0.5438\n",
      "Epoch 143/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6898 - acc: 0.5438\n",
      "Epoch 144/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6898 - acc: 0.5438\n",
      "Epoch 145/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6898 - acc: 0.5438\n",
      "Epoch 146/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6898 - acc: 0.5438\n",
      "Epoch 147/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 148/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 149/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 150/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6898 - acc: 0.5438\n",
      "Epoch 151/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 152/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 153/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 154/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 155/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 156/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 157/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 158/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 159/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 160/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 161/200\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 162/200\n",
      "160/160 [==============================] - 0s 38us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 163/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 164/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6897 - acc: 0.5438\n",
      "Epoch 165/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 166/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 167/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 168/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 169/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 170/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 171/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 172/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 173/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 174/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 175/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 176/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 177/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 178/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 179/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 180/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 181/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 182/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 183/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 184/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 185/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 186/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 187/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 188/200\n",
      "160/160 [==============================] - 0s 43us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 189/200\n",
      "160/160 [==============================] - 0s 40us/step - loss: 0.6895 - acc: 0.5438\n",
      "Epoch 190/200\n",
      "160/160 [==============================] - 0s 44us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 191/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6895 - acc: 0.5438\n",
      "Epoch 192/200\n",
      "160/160 [==============================] - 0s 51us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 193/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 194/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6895 - acc: 0.5438\n",
      "Epoch 195/200\n",
      "160/160 [==============================] - 0s 45us/step - loss: 0.6895 - acc: 0.5438\n",
      "Epoch 196/200\n",
      "160/160 [==============================] - 0s 39us/step - loss: 0.6896 - acc: 0.5438\n",
      "Epoch 197/200\n",
      "160/160 [==============================] - 0s 46us/step - loss: 0.6895 - acc: 0.5438\n",
      "Epoch 198/200\n",
      "160/160 [==============================] - 0s 37us/step - loss: 0.6895 - acc: 0.5438\n",
      "Epoch 199/200\n",
      "160/160 [==============================] - 0s 41us/step - loss: 0.6895 - acc: 0.5438\n",
      "Epoch 200/200\n",
      "160/160 [==============================] - 0s 42us/step - loss: 0.6895 - acc: 0.5438\n",
      "Epoch 1/100\n",
      "320/320 [==============================] - 3s 11ms/step - loss: 0.6931 - acc: 0.5250\n",
      "Epoch 2/100\n",
      "320/320 [==============================] - 0s 214us/step - loss: 0.6928 - acc: 0.5312\n",
      "Epoch 3/100\n",
      "320/320 [==============================] - 0s 205us/step - loss: 0.6925 - acc: 0.5312\n",
      "Epoch 4/100\n",
      "320/320 [==============================] - 0s 213us/step - loss: 0.6923 - acc: 0.5312\n",
      "Epoch 5/100\n",
      "320/320 [==============================] - 0s 215us/step - loss: 0.6921 - acc: 0.5312\n",
      "Epoch 6/100\n",
      "320/320 [==============================] - 0s 211us/step - loss: 0.6920 - acc: 0.5312\n",
      "Epoch 7/100\n",
      "320/320 [==============================] - 0s 202us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 8/100\n",
      "320/320 [==============================] - 0s 203us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 9/100\n",
      "320/320 [==============================] - 0s 209us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 10/100\n",
      "320/320 [==============================] - 0s 221us/step - loss: 0.6917 - acc: 0.5312\n",
      "Epoch 11/100\n",
      "320/320 [==============================] - 0s 211us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 12/100\n",
      "320/320 [==============================] - 0s 211us/step - loss: 0.6915 - acc: 0.5312\n",
      "Epoch 13/100\n",
      "320/320 [==============================] - 0s 223us/step - loss: 0.6915 - acc: 0.5312\n",
      "Epoch 14/100\n",
      "320/320 [==============================] - 0s 205us/step - loss: 0.6915 - acc: 0.5312\n",
      "Epoch 15/100\n",
      "320/320 [==============================] - 0s 222us/step - loss: 0.6915 - acc: 0.5312\n",
      "Epoch 16/100\n",
      "320/320 [==============================] - 0s 206us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 17/100\n",
      "320/320 [==============================] - 0s 207us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 18/100\n",
      "320/320 [==============================] - 0s 208us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 19/100\n",
      "320/320 [==============================] - 0s 207us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 20/100\n",
      "320/320 [==============================] - 0s 211us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 21/100\n",
      "320/320 [==============================] - 0s 207us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 22/100\n",
      "320/320 [==============================] - 0s 212us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 23/100\n",
      "320/320 [==============================] - 0s 203us/step - loss: 0.6915 - acc: 0.5312\n",
      "Epoch 24/100\n",
      "320/320 [==============================] - 0s 215us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 25/100\n",
      "320/320 [==============================] - 0s 199us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 26/100\n",
      "320/320 [==============================] - 0s 215us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 27/100\n",
      "320/320 [==============================] - 0s 204us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 28/100\n",
      "320/320 [==============================] - 0s 228us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 29/100\n",
      "320/320 [==============================] - 0s 207us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 30/100\n",
      "320/320 [==============================] - 0s 202us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 31/100\n",
      "320/320 [==============================] - 0s 213us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 32/100\n",
      "320/320 [==============================] - 0s 201us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 33/100\n",
      "320/320 [==============================] - 0s 199us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 34/100\n",
      "320/320 [==============================] - 0s 201us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 35/100\n",
      "320/320 [==============================] - 0s 202us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 36/100\n",
      "320/320 [==============================] - 0s 208us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 37/100\n",
      "320/320 [==============================] - 0s 201us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 38/100\n",
      "320/320 [==============================] - 0s 241us/step - loss: 0.6914 - acc: 0.5312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100\n",
      "320/320 [==============================] - 0s 226us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 40/100\n",
      "320/320 [==============================] - 0s 205us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 41/100\n",
      "320/320 [==============================] - 0s 204us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 42/100\n",
      "320/320 [==============================] - 0s 204us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 43/100\n",
      "320/320 [==============================] - 0s 204us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 44/100\n",
      "320/320 [==============================] - 0s 209us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 45/100\n",
      "320/320 [==============================] - 0s 211us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 46/100\n",
      "320/320 [==============================] - 0s 206us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 47/100\n",
      "320/320 [==============================] - 0s 206us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 48/100\n",
      "320/320 [==============================] - 0s 208us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 49/100\n",
      "320/320 [==============================] - 0s 213us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 50/100\n",
      "320/320 [==============================] - 0s 210us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 51/100\n",
      "320/320 [==============================] - 0s 210us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 52/100\n",
      "320/320 [==============================] - 0s 213us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 53/100\n",
      "320/320 [==============================] - 0s 212us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 54/100\n",
      "320/320 [==============================] - 0s 210us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 55/100\n",
      "320/320 [==============================] - 0s 215us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 56/100\n",
      "320/320 [==============================] - 0s 247us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 57/100\n",
      "320/320 [==============================] - 0s 218us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 58/100\n",
      "320/320 [==============================] - 0s 212us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 59/100\n",
      "320/320 [==============================] - 0s 212us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 60/100\n",
      "320/320 [==============================] - 0s 218us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 61/100\n",
      "320/320 [==============================] - 0s 220us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 62/100\n",
      "320/320 [==============================] - 0s 224us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 63/100\n",
      "320/320 [==============================] - 0s 218us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 64/100\n",
      "320/320 [==============================] - 0s 227us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 65/100\n",
      "320/320 [==============================] - 0s 216us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 66/100\n",
      "320/320 [==============================] - 0s 220us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 67/100\n",
      "320/320 [==============================] - 0s 217us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 68/100\n",
      "320/320 [==============================] - 0s 219us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 69/100\n",
      "320/320 [==============================] - 0s 226us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 70/100\n",
      "320/320 [==============================] - 0s 230us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 71/100\n",
      "320/320 [==============================] - 0s 226us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 72/100\n",
      "320/320 [==============================] - 0s 224us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 73/100\n",
      "320/320 [==============================] - 0s 228us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 74/100\n",
      "320/320 [==============================] - 0s 231us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 75/100\n",
      "320/320 [==============================] - 0s 225us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 76/100\n",
      "320/320 [==============================] - 0s 223us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 77/100\n",
      "320/320 [==============================] - 0s 228us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 78/100\n",
      "320/320 [==============================] - 0s 222us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 79/100\n",
      "320/320 [==============================] - 0s 229us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 80/100\n",
      "320/320 [==============================] - 0s 231us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 81/100\n",
      "320/320 [==============================] - 0s 224us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 82/100\n",
      "320/320 [==============================] - 0s 224us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 83/100\n",
      "320/320 [==============================] - 0s 224us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 84/100\n",
      "320/320 [==============================] - 0s 224us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 85/100\n",
      "320/320 [==============================] - 0s 231us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 86/100\n",
      "320/320 [==============================] - 0s 228us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 87/100\n",
      "320/320 [==============================] - 0s 229us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 88/100\n",
      "320/320 [==============================] - 0s 234us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 89/100\n",
      "320/320 [==============================] - 0s 237us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 90/100\n",
      "320/320 [==============================] - 0s 230us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 91/100\n",
      "320/320 [==============================] - 0s 237us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 92/100\n",
      "320/320 [==============================] - 0s 232us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 93/100\n",
      "320/320 [==============================] - 0s 235us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 94/100\n",
      "320/320 [==============================] - 0s 232us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 95/100\n",
      "320/320 [==============================] - 0s 232us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 96/100\n",
      "320/320 [==============================] - 0s 229us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 97/100\n",
      "320/320 [==============================] - 0s 231us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 98/100\n",
      "320/320 [==============================] - 0s 232us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 99/100\n",
      "320/320 [==============================] - 0s 231us/step - loss: 0.6914 - acc: 0.5312\n",
      "Epoch 100/100\n",
      "320/320 [==============================] - 0s 230us/step - loss: 0.6913 - acc: 0.5312\n"
     ]
    }
   ],
   "source": [
    "grid_search= grid_search.fit(X_train, y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 8, 'epochs': 100}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can retrain our NN with the following parameters to improve accuracy\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### NN: Using Adam Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sequential class\n",
    "classifier = Sequential()\n",
    "\n",
    "# adding first dense layer with relu activation and 8 hidden units\n",
    "classifier.add(Dense(input_dim = 2, units = 8, kernel_initializer = 'uniform', activation='relu'))\n",
    "\n",
    "# adding second dense layer with relu activation and 4 hidden units\n",
    "classifier.add(Dense(units = 8, activation = 'relu', kernel_initializer = 'uniform'))\n",
    "\n",
    "# adding third dense layer with relu activation and 4 hidden units\n",
    "classifier.add(Dense(units = 4, activation = 'relu', kernel_initializer = 'uniform'))\n",
    "\n",
    "# adding final sigmoind activation dense layer with 1 unit\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer = 'uniform'))\n",
    "\n",
    "# compiling NN model using sgd, with batch size 32\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "320/320 [==============================] - 4s 11ms/step - loss: 0.6931 - acc: 0.5000\n",
      "Epoch 2/100\n",
      "320/320 [==============================] - 0s 73us/step - loss: 0.6930 - acc: 0.5312\n",
      "Epoch 3/100\n",
      "320/320 [==============================] - 0s 73us/step - loss: 0.6929 - acc: 0.5312\n",
      "Epoch 4/100\n",
      "320/320 [==============================] - 0s 69us/step - loss: 0.6928 - acc: 0.5312\n",
      "Epoch 5/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.6927 - acc: 0.5312\n",
      "Epoch 6/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.6926 - acc: 0.5312\n",
      "Epoch 7/100\n",
      "320/320 [==============================] - 0s 66us/step - loss: 0.6925 - acc: 0.5312\n",
      "Epoch 8/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.6924 - acc: 0.5312\n",
      "Epoch 9/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.6923 - acc: 0.5312\n",
      "Epoch 10/100\n",
      "320/320 [==============================] - 0s 61us/step - loss: 0.6922 - acc: 0.5312\n",
      "Epoch 11/100\n",
      "320/320 [==============================] - 0s 71us/step - loss: 0.6922 - acc: 0.5312\n",
      "Epoch 12/100\n",
      "320/320 [==============================] - 0s 73us/step - loss: 0.6919 - acc: 0.5312\n",
      "Epoch 13/100\n",
      "320/320 [==============================] - 0s 71us/step - loss: 0.6918 - acc: 0.5312\n",
      "Epoch 14/100\n",
      "320/320 [==============================] - 0s 78us/step - loss: 0.6916 - acc: 0.5312\n",
      "Epoch 15/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.6913 - acc: 0.5312\n",
      "Epoch 16/100\n",
      "320/320 [==============================] - 0s 69us/step - loss: 0.6910 - acc: 0.5312\n",
      "Epoch 17/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.6906 - acc: 0.5312\n",
      "Epoch 18/100\n",
      "320/320 [==============================] - 0s 65us/step - loss: 0.6900 - acc: 0.5312\n",
      "Epoch 19/100\n",
      "320/320 [==============================] - 0s 68us/step - loss: 0.6892 - acc: 0.5312\n",
      "Epoch 20/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.6884 - acc: 0.5312\n",
      "Epoch 21/100\n",
      "320/320 [==============================] - 0s 65us/step - loss: 0.6872 - acc: 0.5312\n",
      "Epoch 22/100\n",
      "320/320 [==============================] - 0s 68us/step - loss: 0.6857 - acc: 0.5312\n",
      "Epoch 23/100\n",
      "320/320 [==============================] - 0s 69us/step - loss: 0.6842 - acc: 0.5312\n",
      "Epoch 24/100\n",
      "320/320 [==============================] - 0s 75us/step - loss: 0.6824 - acc: 0.5312\n",
      "Epoch 25/100\n",
      "320/320 [==============================] - 0s 73us/step - loss: 0.6803 - acc: 0.5312\n",
      "Epoch 26/100\n",
      "320/320 [==============================] - 0s 64us/step - loss: 0.6787 - acc: 0.5312\n",
      "Epoch 27/100\n",
      "320/320 [==============================] - 0s 76us/step - loss: 0.6773 - acc: 0.5312\n",
      "Epoch 28/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.6751 - acc: 0.5312\n",
      "Epoch 29/100\n",
      "320/320 [==============================] - 0s 70us/step - loss: 0.6733 - acc: 0.5312\n",
      "Epoch 30/100\n",
      "320/320 [==============================] - 0s 70us/step - loss: 0.6714 - acc: 0.5312\n",
      "Epoch 31/100\n",
      "320/320 [==============================] - 0s 65us/step - loss: 0.6693 - acc: 0.5312\n",
      "Epoch 32/100\n",
      "320/320 [==============================] - 0s 64us/step - loss: 0.6677 - acc: 0.5312\n",
      "Epoch 33/100\n",
      "320/320 [==============================] - 0s 76us/step - loss: 0.6650 - acc: 0.5312\n",
      "Epoch 34/100\n",
      "320/320 [==============================] - 0s 70us/step - loss: 0.6627 - acc: 0.5312\n",
      "Epoch 35/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.6599 - acc: 0.5312\n",
      "Epoch 36/100\n",
      "320/320 [==============================] - 0s 73us/step - loss: 0.6571 - acc: 0.5312\n",
      "Epoch 37/100\n",
      "320/320 [==============================] - 0s 70us/step - loss: 0.6544 - acc: 0.5312\n",
      "Epoch 38/100\n",
      "320/320 [==============================] - 0s 63us/step - loss: 0.6507 - acc: 0.5344\n",
      "Epoch 39/100\n",
      "320/320 [==============================] - 0s 71us/step - loss: 0.6470 - acc: 0.5812\n",
      "Epoch 40/100\n",
      "320/320 [==============================] - 0s 66us/step - loss: 0.6434 - acc: 0.6781\n",
      "Epoch 41/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.6394 - acc: 0.7031\n",
      "Epoch 42/100\n",
      "320/320 [==============================] - 0s 69us/step - loss: 0.6351 - acc: 0.7062\n",
      "Epoch 43/100\n",
      "320/320 [==============================] - 0s 64us/step - loss: 0.6303 - acc: 0.7188\n",
      "Epoch 44/100\n",
      "320/320 [==============================] - 0s 73us/step - loss: 0.6262 - acc: 0.7250\n",
      "Epoch 45/100\n",
      "320/320 [==============================] - 0s 72us/step - loss: 0.6220 - acc: 0.7219\n",
      "Epoch 46/100\n",
      "320/320 [==============================] - 0s 81us/step - loss: 0.6163 - acc: 0.7312\n",
      "Epoch 47/100\n",
      "320/320 [==============================] - 0s 72us/step - loss: 0.6113 - acc: 0.7250\n",
      "Epoch 48/100\n",
      "320/320 [==============================] - 0s 65us/step - loss: 0.6062 - acc: 0.7250\n",
      "Epoch 49/100\n",
      "320/320 [==============================] - 0s 65us/step - loss: 0.6010 - acc: 0.7281\n",
      "Epoch 50/100\n",
      "320/320 [==============================] - 0s 69us/step - loss: 0.5954 - acc: 0.7344\n",
      "Epoch 51/100\n",
      "320/320 [==============================] - 0s 71us/step - loss: 0.5904 - acc: 0.7344\n",
      "Epoch 52/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.5856 - acc: 0.7375\n",
      "Epoch 53/100\n",
      "320/320 [==============================] - 0s 65us/step - loss: 0.5795 - acc: 0.7312\n",
      "Epoch 54/100\n",
      "320/320 [==============================] - 0s 73us/step - loss: 0.5751 - acc: 0.7312\n",
      "Epoch 55/100\n",
      "320/320 [==============================] - 0s 69us/step - loss: 0.5695 - acc: 0.7312\n",
      "Epoch 56/100\n",
      "320/320 [==============================] - 0s 64us/step - loss: 0.5640 - acc: 0.7375\n",
      "Epoch 57/100\n",
      "320/320 [==============================] - 0s 63us/step - loss: 0.5601 - acc: 0.7344\n",
      "Epoch 58/100\n",
      "320/320 [==============================] - 0s 64us/step - loss: 0.5542 - acc: 0.7406\n",
      "Epoch 59/100\n",
      "320/320 [==============================] - 0s 64us/step - loss: 0.5485 - acc: 0.7438\n",
      "Epoch 60/100\n",
      "320/320 [==============================] - 0s 68us/step - loss: 0.5432 - acc: 0.7406\n",
      "Epoch 61/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.5376 - acc: 0.7438\n",
      "Epoch 62/100\n",
      "320/320 [==============================] - 0s 62us/step - loss: 0.5324 - acc: 0.7469\n",
      "Epoch 63/100\n",
      "320/320 [==============================] - 0s 66us/step - loss: 0.5275 - acc: 0.7469\n",
      "Epoch 64/100\n",
      "320/320 [==============================] - 0s 66us/step - loss: 0.5220 - acc: 0.7500\n",
      "Epoch 65/100\n",
      "320/320 [==============================] - 0s 63us/step - loss: 0.5181 - acc: 0.7469\n",
      "Epoch 66/100\n",
      "320/320 [==============================] - 0s 63us/step - loss: 0.5122 - acc: 0.7469\n",
      "Epoch 67/100\n",
      "320/320 [==============================] - 0s 66us/step - loss: 0.5075 - acc: 0.7500\n",
      "Epoch 68/100\n",
      "320/320 [==============================] - 0s 66us/step - loss: 0.5026 - acc: 0.7750\n",
      "Epoch 69/100\n",
      "320/320 [==============================] - 0s 72us/step - loss: 0.4981 - acc: 0.7969\n",
      "Epoch 70/100\n",
      "320/320 [==============================] - 0s 65us/step - loss: 0.4947 - acc: 0.8031\n",
      "Epoch 71/100\n",
      "320/320 [==============================] - 0s 71us/step - loss: 0.4900 - acc: 0.8031\n",
      "Epoch 72/100\n",
      "320/320 [==============================] - 0s 64us/step - loss: 0.4856 - acc: 0.8031\n",
      "Epoch 73/100\n",
      "320/320 [==============================] - 0s 74us/step - loss: 0.4818 - acc: 0.8094\n",
      "Epoch 74/100\n",
      "320/320 [==============================] - 0s 74us/step - loss: 0.4763 - acc: 0.8094\n",
      "Epoch 75/100\n",
      "320/320 [==============================] - 0s 68us/step - loss: 0.4714 - acc: 0.8125\n",
      "Epoch 76/100\n",
      "320/320 [==============================] - 0s 71us/step - loss: 0.4676 - acc: 0.8156\n",
      "Epoch 77/100\n",
      "320/320 [==============================] - 0s 69us/step - loss: 0.4633 - acc: 0.8187\n",
      "Epoch 78/100\n",
      "320/320 [==============================] - 0s 63us/step - loss: 0.4593 - acc: 0.8156\n",
      "Epoch 79/100\n",
      "320/320 [==============================] - 0s 68us/step - loss: 0.4557 - acc: 0.8156\n",
      "Epoch 80/100\n",
      "320/320 [==============================] - 0s 64us/step - loss: 0.4533 - acc: 0.8156\n",
      "Epoch 81/100\n",
      "320/320 [==============================] - 0s 70us/step - loss: 0.4491 - acc: 0.8156\n",
      "Epoch 82/100\n",
      "320/320 [==============================] - 0s 66us/step - loss: 0.4457 - acc: 0.8187\n",
      "Epoch 83/100\n",
      "320/320 [==============================] - 0s 64us/step - loss: 0.4422 - acc: 0.8187\n",
      "Epoch 84/100\n",
      "320/320 [==============================] - 0s 72us/step - loss: 0.4392 - acc: 0.8187\n",
      "Epoch 85/100\n",
      "320/320 [==============================] - 0s 66us/step - loss: 0.4365 - acc: 0.8187\n",
      "Epoch 86/100\n",
      "320/320 [==============================] - 0s 64us/step - loss: 0.4339 - acc: 0.8219\n",
      "Epoch 87/100\n",
      "320/320 [==============================] - 0s 67us/step - loss: 0.4315 - acc: 0.8281\n",
      "Epoch 88/100\n",
      "320/320 [==============================] - 0s 63us/step - loss: 0.4288 - acc: 0.8344\n",
      "Epoch 89/100\n",
      "320/320 [==============================] - 0s 60us/step - loss: 0.4261 - acc: 0.8281\n",
      "Epoch 90/100\n",
      "320/320 [==============================] - 0s 62us/step - loss: 0.4241 - acc: 0.8375\n",
      "Epoch 91/100\n",
      "320/320 [==============================] - 0s 64us/step - loss: 0.4217 - acc: 0.8375\n",
      "Epoch 92/100\n",
      "320/320 [==============================] - 0s 69us/step - loss: 0.4193 - acc: 0.8406\n",
      "Epoch 93/100\n",
      "320/320 [==============================] - 0s 66us/step - loss: 0.4174 - acc: 0.8438\n",
      "Epoch 94/100\n",
      "320/320 [==============================] - 0s 69us/step - loss: 0.4154 - acc: 0.8438\n",
      "Epoch 95/100\n",
      "320/320 [==============================] - 0s 68us/step - loss: 0.4135 - acc: 0.8500\n",
      "Epoch 96/100\n",
      "320/320 [==============================] - 0s 69us/step - loss: 0.4117 - acc: 0.8438\n",
      "Epoch 97/100\n",
      "320/320 [==============================] - 0s 71us/step - loss: 0.4097 - acc: 0.8469\n",
      "Epoch 98/100\n",
      "320/320 [==============================] - 0s 63us/step - loss: 0.4076 - acc: 0.8500\n",
      "Epoch 99/100\n",
      "320/320 [==============================] - 0s 71us/step - loss: 0.4064 - acc: 0.8469\n",
      "Epoch 100/100\n",
      "320/320 [==============================] - 0s 72us/step - loss: 0.4041 - acc: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc713279f28>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting ytraining data with batch size and number of epochs\n",
    "classifier.fit(X_train, y_train, batch_size=32, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# changing probabilities to 1 or 0\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 10],\n",
       "       [ 9, 21]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7625"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We observe that convergence is better for Adam Optimizer when compared to Mini-Batch Gradient Descent. In the next post, let us take a look at state of the art optimizers. (https://arxiv.org/pdf/1412.6980.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
