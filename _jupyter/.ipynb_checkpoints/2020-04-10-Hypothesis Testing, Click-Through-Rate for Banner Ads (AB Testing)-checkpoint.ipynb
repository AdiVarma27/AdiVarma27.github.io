{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Click-Through-Rates for Banner Ads (A/B Testing)\n",
    "\n",
    "* Lets say we are a new apparel store; after thorough market research, we decide to open up an <b> Online Apparel Store.</b> We hire Developers, Digital Media Strategists and Data Scientists, who help develop the store, place products and conduct controlled experiments on the website.\n",
    "\n",
    "\n",
    "* Traditionally, companies ran controlled experiments, either A/B Tests or Multivariate tests, based on requirements. <b>Multiple versions of Banner Ads, Text Ads and Video Ads are created, tested and placed on the website. Website layouts, Ad positions, transitions and many other attributes can be tested.</b>\n",
    "\n",
    "\n",
    "* Our version-A (Still in red colored background after the Holiday season), was on our website for 2 months or so, and we think its time for a change. Assuming everything else kept constant, we develop <b>version-B with subtle, earthy colored banner with the same text.</b> \n",
    "\n",
    "\n",
    "### How do we decide if we should go for the switch (replace version-a with version-b) ?</b>\n",
    "\n",
    "### Controlled A/B Test\n",
    "\n",
    "\n",
    "* Content, color, text style, text size, banner location and placement and many other things need to be taken into account when trying to conduct a controlled experiment. If we plan to replace version-A with version-B, we need <b>strong evidence that click-through-rate (clicks/ impression) for version-B is significantly higher than version-A.</b>\n",
    "\n",
    "\n",
    "* Every visitor who visits our homepage, is <b>randomly (with equal probability) going to see either version-A (Older version) or version-B (New creative) on our homepage.</b> We observe, that the older version has a CTR (Click-through-rate) of <b>9 % (9 clicks every 100 impressions).</b> Let us say we have an <b>average of 200 visitors every day (new + returning users).</b>\n",
    "\n",
    "\n",
    "* We assume and test for the hypothesis that our new banner Ad (version-B), can provide some boost to the CTR. 25 % boost would mean an average-CTR of 11.25 % (11.25 clicks every 100 impressions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# CTR previous version\n",
    "ctr_version_a = 0.09\n",
    "\n",
    "# CTR new version with 25 % expected boost\n",
    "ctr_version_b = 0.09 + (0.25)*(0.09)\n",
    "\n",
    "ctr_version_a, ctr_version_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our null hypothesis is that there is no difference in CTR for version a and b, with alternate hypothesis that CTR of version-B sees a boost in CTR. We conduct a Two-Sample Proportion Test to validate our hypotheses.\n",
    "\n",
    "$$H_0: \\mu_b > \\mu_a$$\n",
    "\n",
    "$$H_a: \\mu_b <= \\mu_a $$\n",
    "\n",
    "\n",
    "\n",
    "We know, t-stat is calculated by the following\n",
    "\n",
    "$$t = \\frac{(\\mu_b - \\mu_a) - 0}{SE}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$t = \\frac{(\\mu_b - \\mu_a) - 0}{\\sqrt{\\frac{CTR_b(1-CTR_b)}{N_b} + \\frac{CTR_a(1-CTR_a)}{N_a}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us choose a type-I error rate of 5 % (alpha = 0.05). Now, we simluate the test by sending customers to either of the pages randomly with equal chance. Let us say we start pushing these two version randomly on day 1. On Average, we expect around 200 customers to open the website, of which approximately 100 of them are exposed to version-A, and 100 are exposed to version-B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to flip between version-a and b.\n",
    "def flipVersion(version_a):\n",
    "    if version_a:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Day 1 \n",
    "\n",
    "* After end of day 1, we observe that there were 201 customers who visited the webiste, and 51 customers were shown version-a and another 51 were shown version-b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total customer incoming per day are normally distributed with mean 200 an deviation 10.\n",
    "\n",
    "np.random.seed(25)\n",
    "\n",
    "num_cust = int(np.random.normal(200, 10))\n",
    "\n",
    "# total number of impressions and clicks at start of experiment are zero\n",
    "num_imps_version_a = 0\n",
    "num_imps_version_b = 0\n",
    "\n",
    "num_clicks_version_a = 0\n",
    "num_clicks_version_b = 0\n",
    "\n",
    "# start by showing version-A\n",
    "version_a = True\n",
    "\n",
    "# send each customer to a or b\n",
    "for customer_number in range(num_cust):\n",
    "    \n",
    "    # if version-a is exposed\n",
    "    if version_a is True:\n",
    "        # increase impression count\n",
    "        num_imps_version_a += 1\n",
    "        # binomial sample (1 if successfully clicked, else 0)\n",
    "        num_clicks_version_a += np.random.binomial(1, ctr_version_a)\n",
    "        \n",
    "    # if version-b is exposed \n",
    "    else:\n",
    "        # increase impression count\n",
    "        num_imps_version_b += 1\n",
    "        # binomial sample (1 if successfully clicked, else 0)\n",
    "        num_clicks_version_b += np.random.binomial(1, ctr_version_b)\n",
    "    \n",
    "    # flip version after each customer\n",
    "    version_a = flipVersion(version_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cust, num_imps_version_a, num_imps_version_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clicks_version_a, num_clicks_version_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We observe that 6 customers clicked on version-a, and 12 clicked on version-b. Plugging it into the above t-stat formula, we obtain the following. The Day-1 CTRs after running the experiment are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_day_one_version_a = num_clicks_version_a/num_imps_version_a\n",
    "\n",
    "ctr_day_one_version_b = num_clicks_version_b/num_imps_version_b\n",
    "\n",
    "ctr_day_one_version_a, ctr_day_one_version_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (num_clicks_version_a + num_clicks_version_b)/(num_imps_version_a + num_imps_version_b)\n",
    "\n",
    "SE = np.sqrt(p*(1-p)*( (1/num_imps_version_a)  +  (1/num_imps_version_b) ))\n",
    "\n",
    "p, SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (ctr_day_one_version_b - ctr_day_one_version_a)/(SE)\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After Day-1, we observe the t-stat is 1.48 (We did not find a significant set of observations to conclude that verion-b is better than version-a). \n",
    "\n",
    "### How long do we run the test for ? When do we know exactly that Version-B is better than Version-A ?\n",
    "\n",
    "* In few cases, sample size is pre-defined to control Type-II error along with Type-I error, and once enough samples are collected, choice is made. In few cases, analysis is done over how t-stat improves as samples are collected.\n",
    "\n",
    "\n",
    "* In our case, we can observe how t-stat changes (increases or decreases with time and sample size), and then decide when to stop or continue the experiment. Note that it is always better to estimate the Power and decide on sample size to allocate budgets before the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conductExperiment(n_days):\n",
    "    \n",
    "    list_num_cust = []\n",
    "    list_t_stat = []\n",
    "    list_ctr_version_a, list_ctr_version_b = [], []  \n",
    "    list_imp_version_a, list_imp_version_b = [], []  \n",
    "    \n",
    "    for i in range(0,n_days):\n",
    "\n",
    "        # total customer incoming per day are normally distributed with mean 200 an deviation 10.\n",
    "\n",
    "        num_cust = int(np.random.normal(200,10))\n",
    "        \n",
    "        list_num_cust.append(num_cust)\n",
    "        \n",
    "        # total number of impressions and clicks at start of experiment are zero\n",
    "        num_imps_version_a = 0\n",
    "        num_imps_version_b = 0\n",
    "\n",
    "        num_clicks_version_a = 0\n",
    "        num_clicks_version_b = 0\n",
    "\n",
    "        # start by showing version-A\n",
    "        version_a = True\n",
    "\n",
    "        # send each customer to a or b\n",
    "        for customer_number in range(num_cust):\n",
    "\n",
    "            # if version-a is exposed\n",
    "            if version_a is True:\n",
    "                # increase impression count\n",
    "                num_imps_version_a += 1\n",
    "                # binomial sample (1 if successfully clicked, else 0)\n",
    "                num_clicks_version_a += np.random.binomial(1, ctr_version_a)\n",
    "\n",
    "            # if version-b is exposed \n",
    "            else:\n",
    "                # increase impression count\n",
    "                num_imps_version_b += 1\n",
    "                # binomial sample (1 if successfully clicked, else 0)\n",
    "                num_clicks_version_b += np.random.binomial(1, ctr_version_b)\n",
    "\n",
    "            # flip version after each customer\n",
    "            version_a = flipVersion(version_a)\n",
    "        \n",
    "        ctr_day_one_version_a = num_clicks_version_a/num_imps_version_a\n",
    "\n",
    "        ctr_day_one_version_b = num_clicks_version_b/num_imps_version_b\n",
    "        \n",
    "        list_ctr_version_a.append(ctr_day_one_version_a)\n",
    "        list_ctr_version_b.append(ctr_day_one_version_b)\n",
    "        \n",
    "        list_imp_version_a.append(num_imps_version_a)\n",
    "        list_imp_version_b.append(num_imps_version_b)\n",
    "        \n",
    "    \n",
    "    df_abtest = pd.DataFrame()\n",
    "    df_abtest['num_cust'] = list_num_cust\n",
    "    df_abtest['IMP_version_a'] = list_imp_version_a\n",
    "    df_abtest['IMP_version_b'] = list_imp_version_b\n",
    "    df_abtest['CTR_version_a'] = list_ctr_version_a\n",
    "    df_abtest['CTR_version_b'] = list_ctr_version_b\n",
    "    df_abtest['Clicks_version_b'] = df_abtest['IMP_version_b']*df_abtest['CTR_version_b']\n",
    "    df_abtest['Clicks_version_a'] = df_abtest['IMP_version_a']*df_abtest['CTR_version_a']\n",
    "\n",
    "    \n",
    "    return df_abtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating experiment results for 3 Days \n",
    "\n",
    "* Now, let us simulate the results for first 3-days, we have the impressions and CTRs for both versions. We can calculate a rolling t-statistic, which can help decide if the CTR of version-b outperforms CTR of version-a.\n",
    "\n",
    "\n",
    "* As days pass by and we collect more data, the sample size (N) increases, decreasing the Standard Error term over time (Daily standard error are probably be very close). Conducting t-test on daily level does not make sense, on Day-2, we need to include the numbers from Day-1 and Day-2 as well, and calculate the t-statistics cumulatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abtest = conductExperiment(3)\n",
    "df_abtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below, we re-write the previous function to get cumulative t-stat and Standard Error terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tStatAfterNDays(n_days):\n",
    "\n",
    "    # total customer incoming per day are normally distributed with mean 200 an deviation 10.\n",
    "\n",
    "    np.random.seed(25)\n",
    "    num_cust = 200*n_days\n",
    "\n",
    "    # total number of impressions and clicks at start of experiment are zero\n",
    "    num_imps_version_a = 0\n",
    "    num_imps_version_b = 0\n",
    "\n",
    "    num_clicks_version_a = 0\n",
    "    num_clicks_version_b = 0\n",
    "\n",
    "    # start by showing version-A\n",
    "    version_a = True\n",
    "\n",
    "    # send each customer to a or b\n",
    "    for customer_number in range(num_cust):\n",
    "        \n",
    "        # if version-a is exposed\n",
    "        if version_a is True:\n",
    "            # increase impression count\n",
    "            num_imps_version_a += 1\n",
    "            # binomial sample (1 if successfully clicked, else 0)\n",
    "            num_clicks_version_a += np.random.binomial(1, ctr_version_a)\n",
    "\n",
    "        # if version-b is exposed \n",
    "        else:\n",
    "            # increase impression count\n",
    "            num_imps_version_b += 1\n",
    "            # binomial sample (1 if successfully clicked, else 0)\n",
    "            num_clicks_version_b += np.random.binomial(1, ctr_version_b)\n",
    "\n",
    "        # flip version after each customer\n",
    "        version_a = flipVersion(version_a)\n",
    "\n",
    "    ctr_day_one_version_a = num_clicks_version_a/num_imps_version_a\n",
    "\n",
    "    ctr_day_one_version_b = num_clicks_version_b/num_imps_version_b\n",
    "\n",
    "    p = (num_clicks_version_a + num_clicks_version_b)/num_cust\n",
    "\n",
    "    SE = np.sqrt(p*(1-p)*( (1/num_imps_version_a)  +  (1/num_imps_version_b) ))\n",
    "\n",
    "    t = (ctr_day_one_version_b - ctr_day_one_version_a)/(SE)\n",
    "\n",
    "    return t, SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us simulate the results for 3 consecutive days to obtain cumulative T-stats and Standard Errors. We observe in figure-1, that the cumulative t-stat has an increase gradually and is approximately 1.645 + after day and a half. On the right, we observe that the Standard Errors reduce cumulatively due to increase in sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_consecutive_days = 3\n",
    "\n",
    "ndays = [i for i in range(1, n_consecutive_days + 1)]\n",
    "tStatsCumulative = [tStatAfterNDays(i)[0] for i in range(1,n_consecutive_days + 1)]\n",
    "SEStatsCumulative = [tStatAfterNDays(i)[1] for i in range(1,n_consecutive_days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ndays, tStatsCumulative)\n",
    "plt.grid()\n",
    "plt.title('Cumulative T-Stat')\n",
    "plt.xlabel('Number of Days')\n",
    "plt.ylabel('Cumulative T-Stat')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(ndays, SEStatsCumulative)\n",
    "plt.grid()\n",
    "plt.title('Cumulative SE')\n",
    "plt.xlabel('Number of Days')\n",
    "plt.ylabel('Cumulative SE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observartions:\n",
    "\n",
    "* We observe that after day and a half of both versions up and running, <b>there is a statistically significant difference between CTRs of version-a and version-b, with version-b outperforming version-a.</b>\n",
    "\n",
    "\n",
    "* Could we have <b>stopped the experiment after one and a half days ? Do we know if this effect is consistent on weekends ? Can we attribute these spike in CTR due to these changes only ? These are all design choices and can be decided only with additional Business context. Ideally, we would want to know the effects of weekdays vs weekends. Collecting more samples by experimentation provides deeper understanding of customer behaviour.</b>\n",
    "\n",
    "\n",
    "* Now, let us take a look at how to calculate the sample size required to control for a required Beta (Type-II Error). Note that deciding alpha and beta (Type-I and Type-II Error) rates are design choices as well, and deciding sample size before conducting the experiment is not only a best practice, but also helps decide the approximate Time and Budget it takes to provide confident and conclusive results.\n",
    "\n",
    "\n",
    "### Controlling Power by varying Sample Size\n",
    "\n",
    "* Just like choosing significance level alpha (0.05), we need to choose power (1 - beta), generally chosen around 95 % power (beta = 0.05). First, let us look at the distribution of version-A, cumulatively for 3 days. Our sample size (Number of impressions for version-a) is 600 (3 days x 200 impressions per day). The average CTR for 3 days is 0.09.\n",
    "\n",
    "\n",
    "* Given we know sample size and proportion, we can now calculate the critical cut-off value (cut off proportion).\n",
    "\n",
    "$$p_{crit+} = p_0 + 1.645(SE)$$\n",
    "\n",
    "\n",
    "### Version-a\n",
    "\n",
    "* We observe that 95 % of data lies within 0 and 0.1171 with mean Click-Through-Rate of 0.09."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 100*3\n",
    "ctr_a = 0.09\n",
    "\n",
    "SE = np.sqrt(ctr_a*(1-ctr_a)/n_a)\n",
    "\n",
    "p_crit_a = ctr_a + 1.645*(SE)\n",
    "\n",
    "p_crit_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version-b\n",
    "\n",
    "* Let us assume that version-b, has an average CTR at the critical cutoff value of version-a (0.117).\n",
    "\n",
    "<img src=\"power.png\" width=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type-I and Type-II Errors:\n",
    "\n",
    "* Type-I error corresponds to the green shaded region (alpha=0.05), where we are allowing upto 5 % of sampling data to be misclassified (Assume they come from version-b and not version-a).\n",
    "\n",
    "\n",
    "* Type-II error corresponds to data sampled from version-b, but falls within region of version-a, and hence misclassified (Shaded in red). We observe that exactly half of the version-b falls within rejection region, making 50 % Type-II errors, which is high. \n",
    "\n",
    "\n",
    "* <b>Increasing sampling size can help reduce Type-II error at the same version-b mean, as higher sampling size reduces the standard error, and shrinks the tails of both distributions.</b>\n",
    "\n",
    "\n",
    "### Ideal sample size for alpha (0.05) and beta (0.1)\n",
    "\n",
    "* We can calculate the ideal sample size for constrained alpha and beta parameters. Essentially, the version-b needs to have a mean such that 10 % of data falls out of rejection region, and that needs to be there line where version-a has critical value.\n",
    "\n",
    "<img src=\"power3.png\" width=400></img>\n",
    "\n",
    "* Given we want to control for Type-II error (0.1), with Power of 90 % (1-beta), the Z-stat for 10 % Error rate is 1.29. Hence, for the given sample size, The mean of version-b needs to be atleast 1.29 Z's away from the cutoff value of version-a.\n",
    "\n",
    "$$z_{critical} = \\frac{0.117 - \\mu_b}{SE}$$\n",
    "\n",
    "\n",
    "$$-1.29 = \\frac{0.117 - \\mu_b}{SE}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.143\n",
    "z = (0.117 - p)/(np.sqrt(( p*(1-p))/(300)))\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Therefore, for sample size of 3 days (600 samples, 300 each version), for alpha 0.05 and beta 0.1 (power 0.9), we reach statistical significance if the average click-through-rate of version-b is 14.3 % </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Deciding on sample size before conducting the experiment\n",
    "\n",
    "\n",
    "\n",
    "* Let us construct/ pose the question in an experimental setting. <b>First, lets setup our initial hypothesis for testing. Let us conduct an experiment to test if version-b can provide 50 % boost, when compared to previous version-a.</b>\n",
    "\n",
    "\n",
    "* We are strict with both Type-I and Type-II errors this time, and choose alpha 0.05 and beta 0.05 (0.95 Power). Below is the stated null and alternate hypotheses. We conduct a Two-Sample, One-Tailed Proportion Test to validate our hypotheses.\n",
    "\n",
    "\n",
    "\n",
    "$$H_0: \\mu_b - \\mu_a <= 0.5(\\mu_a)$$\n",
    "\n",
    "$$H_a: \\mu_b - \\mu_a > 0.5(\\mu_a) $$\n",
    "\n",
    "\n",
    "\n",
    "We know, Z-stat is calculated by the following\n",
    "\n",
    "$$z = \\frac{(\\mu_b - \\mu_a) - 0.5(\\mu_a)}{SE}$$\n",
    "\n",
    "\n",
    "To solve for n, we can check it by plugging in Z-values (for null and alternate hypothesis), mean of null and alternate hypothesis. \n",
    "\n",
    "\n",
    "\n",
    "$$ \\mu_0 + z_{b-critical}(\\sqrt{\\frac{p_0(1-p_0)}{n}}) = \\mu_a - z_{a-critical}(\\sqrt{\\frac{p_a(1-p_a)}{n}})$$\n",
    "\n",
    "\n",
    "<b> Knowing sample size in advance can help decide budget, and also provide a good estimate of how long we might have to run the test, to understand which version works better. </b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
